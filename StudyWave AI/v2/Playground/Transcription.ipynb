{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e91cd080",
        "outputId": "22f96fcd-53a2-44b4-ed13-536aba074196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "745a01fc",
        "outputId": "b6560aad-ba89-4a54-9516-71a42cc85e5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title = Unsupervised Learning: Crash Course AI #6\n",
            "Views = 168453\n"
          ]
        }
      ],
      "source": [
        "from pytube import YouTube\n",
        "from sys import argv\n",
        "\n",
        "link = 'https://www.youtube.com/watch?v=JnnaDNNb380'\n",
        "# link = input('Enter video link you want to download : ')\n",
        "yt = YouTube(link)\n",
        "\n",
        "print(\"Title = {}\".format(yt.title))\n",
        "print(\"Views = {}\".format(yt.views))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "580b436b",
        "outputId": "cf28b6ea-ecdc-4b6b-a1b2-5e5e6943d581"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Unsupervised Learning Crash Course AI 6.mp4'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yd = yt.streams.get_lowest_resolution()\n",
        "video_path = yd.download()\n",
        "video_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhEK95g1t29W",
        "outputId": "a3f250a0-8cb4-49bb-b74b-7219a5343023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-xf2c_80o\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-xf2c_80o\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=f142e5147341a580bcfb2a5d17212a7aeee6b12d73bf08e7ec00df7f7afe7912\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mldpknmq/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDvRI7yUtxkN",
        "outputId": "3cbf99f9-ff51-49f3-8f73-0a13965971ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:06<00:00, 75.5MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Whisper model loaded successfully\n",
            "MoviePy - Writing audio in /content/audio.wav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "{'text': \" Thanks to Wix for supporting PBS Digital Studios. Hey, I'm Jabruro and welcome to Crash Course AI. So far in the series, we focused on artificial intelligence that uses supervised learning. These programs need a teacher to use labeled data to tell them right from wrong. And we humans have places where supervised learning happens, like classrooms with teachers. But that's not the only way we learn. We can also learn lots of things on our own by finding patterns in the world. We can look at dogs and elephants and know they're different animals without anyone telling us. Or we can even figure out the rules of a sport just by watching other people play. This kind of learning without a teacher is called unsupervised learning. And in some cases, computers can do it too. The key difference between supervised and unsupervised learning is what we're trying to predict. In supervised learning, we're trying to build a model to predict an answer or label provided by a teacher. In unsupervised learning, instead of a teacher, the world around us is basically providing training labels. For example, if I freeze this video of a tennis ball right now, can you draw what could be the next frame? Unsupervised learning is about modeling the world by guessing like this. And it's useful because we don't need labels provided by a teacher. Babies do a lot of unsupervised learning by watching and imitating people. And we'd like computers to be able to learn like this as well. This lets us utilize lots of freely available data in the world or on the internet. In many cases, one of the easiest ways to understand how AI can use unsupervised learning is by doing it ourselves. So let's look at a few photos of flowers with no labels. The most basic way to model the world is to assume that it's made up of distinct groups of objects that share properties. So, for example, how many types of flowers are here? We can say there are two because there are two colors, purple and yellow. Or we can look at the petal shapes and divide them into round petals and tall vertical ones. Or maybe we have some more experience with flowers and realize that two of these are tulips, one is a sunflower, and the last one is a daisy. So there are three categories. Immediately recognizing different properties like this and creating categories is called unsupervised clustering. We don't have labels provided by a teacher, but we do have a key assumption about the world that we're modeling. Certain objects are more similar to each other than others. We can program computers to perform clustering. But to do that, we need to choose a few properties of flowers we're interested in looking at, like how we picked color or shape just now. For a more realistic example, let's say I bought a packet of iris seeds to plant in my garden. After the flowers bloom, though, it looks like there were several different species of irises mixed up in that one packet. Now, I'm no expert gardener, but I can use some AI to help me analyze my garden. To construct the model, we have to answer two key questions. First, what observation can we measure? All of these flowers are purple, so that's probably not the best way to tell them apart. But different irises seem to have different petal lengths and widths, which we can measure and place on this graph with petal length on the y-axis and width on the x-axis. And second, how do we want to represent the world? We're going to stick to a very simple assumption here. There are clusters in our data. Specifically, we're going to say there are some number of groups called K clusters, but we don't know where they are. To help us, we're going to use the K-means clustering algorithm. K-means clustering is a simple algorithm. All it needs is a way to compare observations, a way to guess how many clusters exist in the data, and a way to calculate averages for each cluster it predicts. In particular, we want to calculate the mean by adding up all data points in a cluster and dividing by the total number of points. Remember, unsupervised learning is about modeling the world, so our algorithm will have two steps. First, our AI will predict. What does the model expect the world to look like? In other words, which flowers should be clustered together because they're the same species? Second, our AI will correct or learn. The model will update its belief to agree with its observation of the world. To start the process, we have to specify how many clusters the model should look for. I'm guessing there are three clusters in the data, so that becomes the model's initial understanding of the world. And we're looking for K equals 3 averages or 3 types of viruses. But to start, our model doesn't really know anything. So the averages are random and so are its predictions. Each data point, which is a flower, is given a label as type 1, type 2, or type 3. Based on the algorithm's beliefs. Next, our model tries to correct itself. The average of each cluster of data points should be in the middle. So the model corrects itself by calculating new averages. We can see those averages here, marked with X's, which gives us an updated model of the three, or so we guessed, types of viruses. But the graph is still pretty noisy. For example, it's a little weird that we have type 2 flowers so close to the average for type 3 flowers. But we did start with a random model, so we can't expect too much accuracy. Logically, we know that irises of the same species tend to have similar petals. So those data points should be clustered together. Since we did a correction or learning step, we can repeat the process, starting with a new prediction step. Let's predict new labels using the X's that mark the averages of each label. We'll give each data point the label of its closest X. Type 1, type 2, or type 3. And then we'll calculate new averages. Ah, that's better. But still not the cleanest clusters. So we can repeat the process again. Predict, learn, predict, learn. Eventually, the X's will stop moving and we will have a model of iris clusters created with unsupervised learning. Now, the ultimate question is, did we find meaningful patterns about the world with our AI? We made an assumption that there were three types of irises and we assumed that they had different petal lengths and widths. Was this true? Lucky for us, I have a friend who is a master gardener. I showed him the real life flowers closest to each of the three averages and he said that type 1 is versicolor, type 2 is satosa, and type 3 is virginica. Three different iris species. We learned about the world from observation, which is what makes this unsupervised learning. Even though we relied a tiny bit on a teacher, the master gardener, for confirmation and help. Now that we've learned the basics, we can experiment with harder examples. Let's say we want to use unsupervised learning to sort a bunch of different photos, and not just three iris species. First, what observations can we measure? How much green there is, whether there's a nose and fur? To have a computer make these observations, we need to measure thousands of red, green, and blue pixels in each image. Second, how do we want to represent the world? Before, we were only working with two features, so we could just use the averages of the cluster data points and get meaningful extraction from it. But when dealing with images, we can't use the same method, because we won't get much meaning out of averaging colored pixels for what we want to accomplish. Somehow, we need the model to create a representation that tells us if two images are similar. There are meaningful patterns in the data that are more abstract than individual pixels, and finding them across many images is what's called representation learning. These patterns help us understand what's in the images and how to compare them to each other. Representation learning happens both in supervised and unsupervised learning models, so we can do it with or without labels to find patterns in the world. To understand the basic idea of representation learning, check out this experiment. I'm going to look at a picture really fast and then try and draw it. Ready, set, go. My eyes took in the picture and remembered important features, so I'm building a representation in my mind. But I can't just show you my thoughts to get feedback on what parts I misremembered, so I have to produce a reconstruction or draw the original image from my memory. Alright, so this is what I've got. Now let's compare my drawing to the original image. Let's see, round plate, triangle slice of pizza, some cheese, some crust, tablecloth. Pretty good. For an AI, making a reconstruction would mean producing all the right pixel values to make a reconstruction. Arcane means clustering algorithm from before predicted classes for flowers based on how close the data points were to the averages. For images, we will have learned image representations instead of averages. After that step, just like before, the AI will have to correct itself. Previously, we updated the K clusters based on how well our predicted labels fit the data, but for images, we'd have to update the model's internal representations based on its reconstructions. There are different ways to use unsupervised learning in combination with representation learning so that an AI can compare images. Like for example, there's a type of neural network called an autoencoder, which uses the same basic principles of weights and biases to process inputs, pass data onto the hidden layers, and finally to a prediction output layer. If John Greenbott was programmed with an autoencoder, the input would be an image, the hidden layers would contain representations, and the output would be a full reconstruction of the original image. Which gets more accurate the more we train this AI. Theoretically, I could give John Greenbott a representation of a pizza and he could reconstruct the original pizza image. What's so powerful about unsupervised learning is that the world is our teacher. By looking around, taking in a lot of data, and predicting what we'll see next, we learn about how the world works and how it should be represented. When asked how AI will fulfill its grand ambitions, 2018 Turing Award winning professor, Jan LeCun said, we all know that unsupervised learning is the ultimate answer. So, I guess we better keep working on it. Unsupervised learning is a huge area of active research. The human brain is specially designed for this kind of learning and has different parts for vision, language, movement, and so on. These structures and what kinds of patterns our brains look for were developed over billions of years of evolution. But it's really tricky to build an AI that does unsupervised learning well, because AI systems can't learn exactly like humans often do, just by watching and imitating. Someone like us has to design the models and tell them how to look for patterns before turning them loose. Next time, we'll look at applying similar concepts to AI systems that find patterns in words and language and what's called natural language processing. See you then. Thanks to Wix for supporting PBS Digital Studios. Check out Wix.com if you're looking to make your own website. Wix is a platform that allows you to build a personalized website for almost any purpose, from promoting your business or creating an online shop to giving you a place for you to test out new ideas. Their technology allows you to create something unique no matter your skill level with templates and all-in-one management. If you'd like to check it out, you can go to wix.com.com. Go slash crash course or click the link in the description. Crash Course AI is produced in association with PBS Digital Studios. If you want to help keep Crash Course free for everyone forever, you can join our community on Patreon. And if you want to learn more about the math behind K-Means clustering, check out this video from Crash Course Statistics.\", 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 3.44, 'text': ' Thanks to Wix for supporting PBS Digital Studios.', 'tokens': [50364, 2561, 281, 343, 970, 337, 7231, 33517, 15522, 23005, 13, 50536], 'temperature': 0.0, 'avg_logprob': -0.12828941512526126, 'compression_ratio': 1.5880398671096345, 'no_speech_prob': 0.0047530923038721085}, {'id': 1, 'seek': 0, 'start': 3.44, 'end': 7.36, 'text': \" Hey, I'm Jabruro and welcome to Crash Course AI.\", 'tokens': [50536, 1911, 11, 286, 478, 40319, 894, 340, 293, 2928, 281, 31787, 27327, 7318, 13, 50732], 'temperature': 0.0, 'avg_logprob': -0.12828941512526126, 'compression_ratio': 1.5880398671096345, 'no_speech_prob': 0.0047530923038721085}, {'id': 2, 'seek': 0, 'start': 7.36, 'end': 12.8, 'text': ' So far in the series, we focused on artificial intelligence that uses supervised learning.', 'tokens': [50732, 407, 1400, 294, 264, 2638, 11, 321, 5178, 322, 11677, 7599, 300, 4960, 46533, 2539, 13, 51004], 'temperature': 0.0, 'avg_logprob': -0.12828941512526126, 'compression_ratio': 1.5880398671096345, 'no_speech_prob': 0.0047530923038721085}, {'id': 3, 'seek': 0, 'start': 12.8, 'end': 17.52, 'text': ' These programs need a teacher to use labeled data to tell them right from wrong.', 'tokens': [51004, 1981, 4268, 643, 257, 5027, 281, 764, 21335, 1412, 281, 980, 552, 558, 490, 2085, 13, 51240], 'temperature': 0.0, 'avg_logprob': -0.12828941512526126, 'compression_ratio': 1.5880398671096345, 'no_speech_prob': 0.0047530923038721085}, {'id': 4, 'seek': 0, 'start': 17.52, 'end': 22.56, 'text': ' And we humans have places where supervised learning happens, like classrooms with teachers.', 'tokens': [51240, 400, 321, 6255, 362, 3190, 689, 46533, 2539, 2314, 11, 411, 22890, 365, 6023, 13, 51492], 'temperature': 0.0, 'avg_logprob': -0.12828941512526126, 'compression_ratio': 1.5880398671096345, 'no_speech_prob': 0.0047530923038721085}, {'id': 5, 'seek': 0, 'start': 22.56, 'end': 24.88, 'text': \" But that's not the only way we learn.\", 'tokens': [51492, 583, 300, 311, 406, 264, 787, 636, 321, 1466, 13, 51608], 'temperature': 0.0, 'avg_logprob': -0.12828941512526126, 'compression_ratio': 1.5880398671096345, 'no_speech_prob': 0.0047530923038721085}, {'id': 6, 'seek': 0, 'start': 24.88, 'end': 28.8, 'text': ' We can also learn lots of things on our own by finding patterns in the world.', 'tokens': [51608, 492, 393, 611, 1466, 3195, 295, 721, 322, 527, 1065, 538, 5006, 8294, 294, 264, 1002, 13, 51804], 'temperature': 0.0, 'avg_logprob': -0.12828941512526126, 'compression_ratio': 1.5880398671096345, 'no_speech_prob': 0.0047530923038721085}, {'id': 7, 'seek': 2880, 'start': 28.8, 'end': 33.52, 'text': \" We can look at dogs and elephants and know they're different animals without anyone telling us.\", 'tokens': [50364, 492, 393, 574, 412, 7197, 293, 33015, 293, 458, 436, 434, 819, 4882, 1553, 2878, 3585, 505, 13, 50600], 'temperature': 0.0, 'avg_logprob': -0.0959187561357525, 'compression_ratio': 1.5051020408163265, 'no_speech_prob': 0.0008829376893118024}, {'id': 8, 'seek': 2880, 'start': 33.52, 'end': 38.08, 'text': ' Or we can even figure out the rules of a sport just by watching other people play.', 'tokens': [50600, 1610, 321, 393, 754, 2573, 484, 264, 4474, 295, 257, 7282, 445, 538, 1976, 661, 561, 862, 13, 50828], 'temperature': 0.0, 'avg_logprob': -0.0959187561357525, 'compression_ratio': 1.5051020408163265, 'no_speech_prob': 0.0008829376893118024}, {'id': 9, 'seek': 2880, 'start': 38.08, 'end': 42.72, 'text': ' This kind of learning without a teacher is called unsupervised learning.', 'tokens': [50828, 639, 733, 295, 2539, 1553, 257, 5027, 307, 1219, 2693, 12879, 24420, 2539, 13, 51060], 'temperature': 0.0, 'avg_logprob': -0.0959187561357525, 'compression_ratio': 1.5051020408163265, 'no_speech_prob': 0.0008829376893118024}, {'id': 10, 'seek': 2880, 'start': 42.72, 'end': 45.760000000000005, 'text': ' And in some cases, computers can do it too.', 'tokens': [51060, 400, 294, 512, 3331, 11, 10807, 393, 360, 309, 886, 13, 51212], 'temperature': 0.0, 'avg_logprob': -0.0959187561357525, 'compression_ratio': 1.5051020408163265, 'no_speech_prob': 0.0008829376893118024}, {'id': 11, 'seek': 4576, 'start': 45.76, 'end': 59.76, 'text': \" The key difference between supervised and unsupervised learning is what we're trying to predict.\", 'tokens': [50364, 440, 2141, 2649, 1296, 46533, 293, 2693, 12879, 24420, 2539, 307, 437, 321, 434, 1382, 281, 6069, 13, 51064], 'temperature': 0.0, 'avg_logprob': -0.1139742740686389, 'compression_ratio': 1.7873563218390804, 'no_speech_prob': 0.030209500342607498}, {'id': 12, 'seek': 4576, 'start': 59.76, 'end': 65.68, 'text': \" In supervised learning, we're trying to build a model to predict an answer or label provided by a teacher.\", 'tokens': [51064, 682, 46533, 2539, 11, 321, 434, 1382, 281, 1322, 257, 2316, 281, 6069, 364, 1867, 420, 7645, 5649, 538, 257, 5027, 13, 51360], 'temperature': 0.0, 'avg_logprob': -0.1139742740686389, 'compression_ratio': 1.7873563218390804, 'no_speech_prob': 0.030209500342607498}, {'id': 13, 'seek': 4576, 'start': 65.68, 'end': 71.68, 'text': ' In unsupervised learning, instead of a teacher, the world around us is basically providing training labels.', 'tokens': [51360, 682, 2693, 12879, 24420, 2539, 11, 2602, 295, 257, 5027, 11, 264, 1002, 926, 505, 307, 1936, 6530, 3097, 16949, 13, 51660], 'temperature': 0.0, 'avg_logprob': -0.1139742740686389, 'compression_ratio': 1.7873563218390804, 'no_speech_prob': 0.030209500342607498}, {'id': 14, 'seek': 7168, 'start': 71.68, 'end': 75.92, 'text': ' For example, if I freeze this video of a tennis ball right now,', 'tokens': [50364, 1171, 1365, 11, 498, 286, 15959, 341, 960, 295, 257, 18118, 2594, 558, 586, 11, 50576], 'temperature': 0.0, 'avg_logprob': -0.04073875090655159, 'compression_ratio': 1.6810035842293907, 'no_speech_prob': 0.00232296041212976}, {'id': 15, 'seek': 7168, 'start': 75.92, 'end': 77.76, 'text': ' can you draw what could be the next frame?', 'tokens': [50576, 393, 291, 2642, 437, 727, 312, 264, 958, 3920, 30, 50668], 'temperature': 0.0, 'avg_logprob': -0.04073875090655159, 'compression_ratio': 1.6810035842293907, 'no_speech_prob': 0.00232296041212976}, {'id': 16, 'seek': 7168, 'start': 77.76, 'end': 81.52000000000001, 'text': ' Unsupervised learning is about modeling the world by guessing like this.', 'tokens': [50668, 25017, 12879, 24420, 2539, 307, 466, 15983, 264, 1002, 538, 17939, 411, 341, 13, 50856], 'temperature': 0.0, 'avg_logprob': -0.04073875090655159, 'compression_ratio': 1.6810035842293907, 'no_speech_prob': 0.00232296041212976}, {'id': 17, 'seek': 7168, 'start': 81.52000000000001, 'end': 85.2, 'text': \" And it's useful because we don't need labels provided by a teacher.\", 'tokens': [50856, 400, 309, 311, 4420, 570, 321, 500, 380, 643, 16949, 5649, 538, 257, 5027, 13, 51040], 'temperature': 0.0, 'avg_logprob': -0.04073875090655159, 'compression_ratio': 1.6810035842293907, 'no_speech_prob': 0.00232296041212976}, {'id': 18, 'seek': 7168, 'start': 85.2, 'end': 89.68, 'text': ' Babies do a lot of unsupervised learning by watching and imitating people.', 'tokens': [51040, 15820, 530, 360, 257, 688, 295, 2693, 12879, 24420, 2539, 538, 1976, 293, 566, 16350, 561, 13, 51264], 'temperature': 0.0, 'avg_logprob': -0.04073875090655159, 'compression_ratio': 1.6810035842293907, 'no_speech_prob': 0.00232296041212976}, {'id': 19, 'seek': 7168, 'start': 89.68, 'end': 92.64000000000001, 'text': \" And we'd like computers to be able to learn like this as well.\", 'tokens': [51264, 400, 321, 1116, 411, 10807, 281, 312, 1075, 281, 1466, 411, 341, 382, 731, 13, 51412], 'temperature': 0.0, 'avg_logprob': -0.04073875090655159, 'compression_ratio': 1.6810035842293907, 'no_speech_prob': 0.00232296041212976}, {'id': 20, 'seek': 7168, 'start': 93.28, 'end': 98.56, 'text': ' This lets us utilize lots of freely available data in the world or on the internet.', 'tokens': [51444, 639, 6653, 505, 16117, 3195, 295, 16433, 2435, 1412, 294, 264, 1002, 420, 322, 264, 4705, 13, 51708], 'temperature': 0.0, 'avg_logprob': -0.04073875090655159, 'compression_ratio': 1.6810035842293907, 'no_speech_prob': 0.00232296041212976}, {'id': 21, 'seek': 9856, 'start': 98.56, 'end': 103.76, 'text': ' In many cases, one of the easiest ways to understand how AI can use unsupervised learning', 'tokens': [50364, 682, 867, 3331, 11, 472, 295, 264, 12889, 2098, 281, 1223, 577, 7318, 393, 764, 2693, 12879, 24420, 2539, 50624], 'temperature': 0.0, 'avg_logprob': -0.07568672830744307, 'compression_ratio': 1.6907894736842106, 'no_speech_prob': 0.00024536249111406505}, {'id': 22, 'seek': 9856, 'start': 103.76, 'end': 105.76, 'text': ' is by doing it ourselves.', 'tokens': [50624, 307, 538, 884, 309, 4175, 13, 50724], 'temperature': 0.0, 'avg_logprob': -0.07568672830744307, 'compression_ratio': 1.6907894736842106, 'no_speech_prob': 0.00024536249111406505}, {'id': 23, 'seek': 9856, 'start': 105.76, 'end': 109.04, 'text': \" So let's look at a few photos of flowers with no labels.\", 'tokens': [50724, 407, 718, 311, 574, 412, 257, 1326, 5787, 295, 8085, 365, 572, 16949, 13, 50888], 'temperature': 0.0, 'avg_logprob': -0.07568672830744307, 'compression_ratio': 1.6907894736842106, 'no_speech_prob': 0.00024536249111406505}, {'id': 24, 'seek': 9856, 'start': 109.04, 'end': 115.28, 'text': \" The most basic way to model the world is to assume that it's made up of distinct groups of objects that share properties.\", 'tokens': [50888, 440, 881, 3875, 636, 281, 2316, 264, 1002, 307, 281, 6552, 300, 309, 311, 1027, 493, 295, 10644, 3935, 295, 6565, 300, 2073, 7221, 13, 51200], 'temperature': 0.0, 'avg_logprob': -0.07568672830744307, 'compression_ratio': 1.6907894736842106, 'no_speech_prob': 0.00024536249111406505}, {'id': 25, 'seek': 9856, 'start': 115.84, 'end': 119.04, 'text': ' So, for example, how many types of flowers are here?', 'tokens': [51228, 407, 11, 337, 1365, 11, 577, 867, 3467, 295, 8085, 366, 510, 30, 51388], 'temperature': 0.0, 'avg_logprob': -0.07568672830744307, 'compression_ratio': 1.6907894736842106, 'no_speech_prob': 0.00024536249111406505}, {'id': 26, 'seek': 9856, 'start': 119.04, 'end': 123.28, 'text': ' We can say there are two because there are two colors, purple and yellow.', 'tokens': [51388, 492, 393, 584, 456, 366, 732, 570, 456, 366, 732, 4577, 11, 9656, 293, 5566, 13, 51600], 'temperature': 0.0, 'avg_logprob': -0.07568672830744307, 'compression_ratio': 1.6907894736842106, 'no_speech_prob': 0.00024536249111406505}, {'id': 27, 'seek': 9856, 'start': 123.28, 'end': 127.76, 'text': ' Or we can look at the petal shapes and divide them into round petals and tall vertical ones.', 'tokens': [51600, 1610, 321, 393, 574, 412, 264, 3817, 304, 10854, 293, 9845, 552, 666, 3098, 31530, 293, 6764, 9429, 2306, 13, 51824], 'temperature': 0.0, 'avg_logprob': -0.07568672830744307, 'compression_ratio': 1.6907894736842106, 'no_speech_prob': 0.00024536249111406505}, {'id': 28, 'seek': 12776, 'start': 127.76, 'end': 132.16, 'text': ' Or maybe we have some more experience with flowers and realize that two of these are tulips,', 'tokens': [50364, 1610, 1310, 321, 362, 512, 544, 1752, 365, 8085, 293, 4325, 300, 732, 295, 613, 366, 30210, 2600, 11, 50584], 'temperature': 0.0, 'avg_logprob': -0.07800560323601095, 'compression_ratio': 1.6732673267326732, 'no_speech_prob': 2.4299950382555835e-05}, {'id': 29, 'seek': 12776, 'start': 132.16, 'end': 134.96, 'text': ' one is a sunflower, and the last one is a daisy.', 'tokens': [50584, 472, 307, 257, 48215, 11, 293, 264, 1036, 472, 307, 257, 1120, 14169, 13, 50724], 'temperature': 0.0, 'avg_logprob': -0.07800560323601095, 'compression_ratio': 1.6732673267326732, 'no_speech_prob': 2.4299950382555835e-05}, {'id': 30, 'seek': 12776, 'start': 134.96, 'end': 136.88, 'text': ' So there are three categories.', 'tokens': [50724, 407, 456, 366, 1045, 10479, 13, 50820], 'temperature': 0.0, 'avg_logprob': -0.07800560323601095, 'compression_ratio': 1.6732673267326732, 'no_speech_prob': 2.4299950382555835e-05}, {'id': 31, 'seek': 12776, 'start': 136.88, 'end': 143.04000000000002, 'text': ' Immediately recognizing different properties like this and creating categories is called unsupervised clustering.', 'tokens': [50820, 34457, 18538, 819, 7221, 411, 341, 293, 4084, 10479, 307, 1219, 2693, 12879, 24420, 596, 48673, 13, 51128], 'temperature': 0.0, 'avg_logprob': -0.07800560323601095, 'compression_ratio': 1.6732673267326732, 'no_speech_prob': 2.4299950382555835e-05}, {'id': 32, 'seek': 12776, 'start': 143.52, 'end': 149.84, 'text': \" We don't have labels provided by a teacher, but we do have a key assumption about the world that we're modeling.\", 'tokens': [51152, 492, 500, 380, 362, 16949, 5649, 538, 257, 5027, 11, 457, 321, 360, 362, 257, 2141, 15302, 466, 264, 1002, 300, 321, 434, 15983, 13, 51468], 'temperature': 0.0, 'avg_logprob': -0.07800560323601095, 'compression_ratio': 1.6732673267326732, 'no_speech_prob': 2.4299950382555835e-05}, {'id': 33, 'seek': 12776, 'start': 149.84, 'end': 152.88, 'text': ' Certain objects are more similar to each other than others.', 'tokens': [51468, 13407, 6565, 366, 544, 2531, 281, 1184, 661, 813, 2357, 13, 51620], 'temperature': 0.0, 'avg_logprob': -0.07800560323601095, 'compression_ratio': 1.6732673267326732, 'no_speech_prob': 2.4299950382555835e-05}, {'id': 34, 'seek': 12776, 'start': 152.88, 'end': 155.76, 'text': ' We can program computers to perform clustering.', 'tokens': [51620, 492, 393, 1461, 10807, 281, 2042, 596, 48673, 13, 51764], 'temperature': 0.0, 'avg_logprob': -0.07800560323601095, 'compression_ratio': 1.6732673267326732, 'no_speech_prob': 2.4299950382555835e-05}, {'id': 35, 'seek': 15576, 'start': 155.76, 'end': 160.95999999999998, 'text': \" But to do that, we need to choose a few properties of flowers we're interested in looking at,\", 'tokens': [50364, 583, 281, 360, 300, 11, 321, 643, 281, 2826, 257, 1326, 7221, 295, 8085, 321, 434, 3102, 294, 1237, 412, 11, 50624], 'temperature': 0.0, 'avg_logprob': -0.08100947737693787, 'compression_ratio': 1.5980707395498392, 'no_speech_prob': 0.0001355187559965998}, {'id': 36, 'seek': 15576, 'start': 160.95999999999998, 'end': 163.51999999999998, 'text': ' like how we picked color or shape just now.', 'tokens': [50624, 411, 577, 321, 6183, 2017, 420, 3909, 445, 586, 13, 50752], 'temperature': 0.0, 'avg_logprob': -0.08100947737693787, 'compression_ratio': 1.5980707395498392, 'no_speech_prob': 0.0001355187559965998}, {'id': 37, 'seek': 15576, 'start': 163.51999999999998, 'end': 168.88, 'text': \" For a more realistic example, let's say I bought a packet of iris seeds to plant in my garden.\", 'tokens': [50752, 1171, 257, 544, 12465, 1365, 11, 718, 311, 584, 286, 4243, 257, 20300, 295, 3418, 271, 9203, 281, 3709, 294, 452, 7431, 13, 51020], 'temperature': 0.0, 'avg_logprob': -0.08100947737693787, 'compression_ratio': 1.5980707395498392, 'no_speech_prob': 0.0001355187559965998}, {'id': 38, 'seek': 15576, 'start': 168.88, 'end': 175.35999999999999, 'text': ' After the flowers bloom, though, it looks like there were several different species of irises mixed up in that one packet.', 'tokens': [51020, 2381, 264, 8085, 26899, 11, 1673, 11, 309, 1542, 411, 456, 645, 2940, 819, 6172, 295, 3418, 3598, 7467, 493, 294, 300, 472, 20300, 13, 51344], 'temperature': 0.0, 'avg_logprob': -0.08100947737693787, 'compression_ratio': 1.5980707395498392, 'no_speech_prob': 0.0001355187559965998}, {'id': 39, 'seek': 15576, 'start': 175.92, 'end': 180.88, 'text': \" Now, I'm no expert gardener, but I can use some AI to help me analyze my garden.\", 'tokens': [51372, 823, 11, 286, 478, 572, 5844, 7431, 260, 11, 457, 286, 393, 764, 512, 7318, 281, 854, 385, 12477, 452, 7431, 13, 51620], 'temperature': 0.0, 'avg_logprob': -0.08100947737693787, 'compression_ratio': 1.5980707395498392, 'no_speech_prob': 0.0001355187559965998}, {'id': 40, 'seek': 15576, 'start': 180.88, 'end': 185.12, 'text': ' To construct the model, we have to answer two key questions.', 'tokens': [51620, 1407, 7690, 264, 2316, 11, 321, 362, 281, 1867, 732, 2141, 1651, 13, 51832], 'temperature': 0.0, 'avg_logprob': -0.08100947737693787, 'compression_ratio': 1.5980707395498392, 'no_speech_prob': 0.0001355187559965998}, {'id': 41, 'seek': 18512, 'start': 185.12, 'end': 187.76, 'text': ' First, what observation can we measure?', 'tokens': [50364, 2386, 11, 437, 14816, 393, 321, 3481, 30, 50496], 'temperature': 0.0, 'avg_logprob': -0.06356623254973313, 'compression_ratio': 1.6282527881040891, 'no_speech_prob': 0.00023048685397952795}, {'id': 42, 'seek': 18512, 'start': 187.76, 'end': 192.8, 'text': \" All of these flowers are purple, so that's probably not the best way to tell them apart.\", 'tokens': [50496, 1057, 295, 613, 8085, 366, 9656, 11, 370, 300, 311, 1391, 406, 264, 1151, 636, 281, 980, 552, 4936, 13, 50748], 'temperature': 0.0, 'avg_logprob': -0.06356623254973313, 'compression_ratio': 1.6282527881040891, 'no_speech_prob': 0.00023048685397952795}, {'id': 43, 'seek': 18512, 'start': 193.36, 'end': 197.36, 'text': ' But different irises seem to have different petal lengths and widths,', 'tokens': [50776, 583, 819, 3418, 3598, 1643, 281, 362, 819, 3817, 304, 26329, 293, 11402, 82, 11, 50976], 'temperature': 0.0, 'avg_logprob': -0.06356623254973313, 'compression_ratio': 1.6282527881040891, 'no_speech_prob': 0.00023048685397952795}, {'id': 44, 'seek': 18512, 'start': 197.92000000000002, 'end': 204.8, 'text': ' which we can measure and place on this graph with petal length on the y-axis and width on the x-axis.', 'tokens': [51004, 597, 321, 393, 3481, 293, 1081, 322, 341, 4295, 365, 3817, 304, 4641, 322, 264, 288, 12, 24633, 293, 11402, 322, 264, 2031, 12, 24633, 13, 51348], 'temperature': 0.0, 'avg_logprob': -0.06356623254973313, 'compression_ratio': 1.6282527881040891, 'no_speech_prob': 0.00023048685397952795}, {'id': 45, 'seek': 18512, 'start': 204.8, 'end': 207.76, 'text': ' And second, how do we want to represent the world?', 'tokens': [51348, 400, 1150, 11, 577, 360, 321, 528, 281, 2906, 264, 1002, 30, 51496], 'temperature': 0.0, 'avg_logprob': -0.06356623254973313, 'compression_ratio': 1.6282527881040891, 'no_speech_prob': 0.00023048685397952795}, {'id': 46, 'seek': 18512, 'start': 207.76, 'end': 210.88, 'text': \" We're going to stick to a very simple assumption here.\", 'tokens': [51496, 492, 434, 516, 281, 2897, 281, 257, 588, 2199, 15302, 510, 13, 51652], 'temperature': 0.0, 'avg_logprob': -0.06356623254973313, 'compression_ratio': 1.6282527881040891, 'no_speech_prob': 0.00023048685397952795}, {'id': 47, 'seek': 18512, 'start': 210.88, 'end': 213.12, 'text': ' There are clusters in our data.', 'tokens': [51652, 821, 366, 23313, 294, 527, 1412, 13, 51764], 'temperature': 0.0, 'avg_logprob': -0.06356623254973313, 'compression_ratio': 1.6282527881040891, 'no_speech_prob': 0.00023048685397952795}, {'id': 48, 'seek': 21312, 'start': 213.12, 'end': 217.92000000000002, 'text': \" Specifically, we're going to say there are some number of groups called K clusters,\", 'tokens': [50364, 26058, 11, 321, 434, 516, 281, 584, 456, 366, 512, 1230, 295, 3935, 1219, 591, 23313, 11, 50604], 'temperature': 0.0, 'avg_logprob': -0.05962836553180029, 'compression_ratio': 1.8084291187739463, 'no_speech_prob': 0.00013551507436204702}, {'id': 49, 'seek': 21312, 'start': 217.92000000000002, 'end': 220.48000000000002, 'text': \" but we don't know where they are.\", 'tokens': [50604, 457, 321, 500, 380, 458, 689, 436, 366, 13, 50732], 'temperature': 0.0, 'avg_logprob': -0.05962836553180029, 'compression_ratio': 1.8084291187739463, 'no_speech_prob': 0.00013551507436204702}, {'id': 50, 'seek': 21312, 'start': 220.48000000000002, 'end': 224.8, 'text': \" To help us, we're going to use the K-means clustering algorithm.\", 'tokens': [50732, 1407, 854, 505, 11, 321, 434, 516, 281, 764, 264, 591, 12, 1398, 599, 596, 48673, 9284, 13, 50948], 'temperature': 0.0, 'avg_logprob': -0.05962836553180029, 'compression_ratio': 1.8084291187739463, 'no_speech_prob': 0.00013551507436204702}, {'id': 51, 'seek': 21312, 'start': 224.8, 'end': 227.68, 'text': ' K-means clustering is a simple algorithm.', 'tokens': [50948, 591, 12, 1398, 599, 596, 48673, 307, 257, 2199, 9284, 13, 51092], 'temperature': 0.0, 'avg_logprob': -0.05962836553180029, 'compression_ratio': 1.8084291187739463, 'no_speech_prob': 0.00013551507436204702}, {'id': 52, 'seek': 21312, 'start': 227.68, 'end': 230.24, 'text': ' All it needs is a way to compare observations,', 'tokens': [51092, 1057, 309, 2203, 307, 257, 636, 281, 6794, 18163, 11, 51220], 'temperature': 0.0, 'avg_logprob': -0.05962836553180029, 'compression_ratio': 1.8084291187739463, 'no_speech_prob': 0.00013551507436204702}, {'id': 53, 'seek': 21312, 'start': 230.24, 'end': 233.12, 'text': ' a way to guess how many clusters exist in the data,', 'tokens': [51220, 257, 636, 281, 2041, 577, 867, 23313, 2514, 294, 264, 1412, 11, 51364], 'temperature': 0.0, 'avg_logprob': -0.05962836553180029, 'compression_ratio': 1.8084291187739463, 'no_speech_prob': 0.00013551507436204702}, {'id': 54, 'seek': 21312, 'start': 233.12, 'end': 236.56, 'text': ' and a way to calculate averages for each cluster it predicts.', 'tokens': [51364, 293, 257, 636, 281, 8873, 42257, 337, 1184, 13630, 309, 6069, 82, 13, 51536], 'temperature': 0.0, 'avg_logprob': -0.05962836553180029, 'compression_ratio': 1.8084291187739463, 'no_speech_prob': 0.00013551507436204702}, {'id': 55, 'seek': 21312, 'start': 237.12, 'end': 242.4, 'text': ' In particular, we want to calculate the mean by adding up all data points in a cluster', 'tokens': [51564, 682, 1729, 11, 321, 528, 281, 8873, 264, 914, 538, 5127, 493, 439, 1412, 2793, 294, 257, 13630, 51828], 'temperature': 0.0, 'avg_logprob': -0.05962836553180029, 'compression_ratio': 1.8084291187739463, 'no_speech_prob': 0.00013551507436204702}, {'id': 56, 'seek': 24240, 'start': 242.4, 'end': 245.04, 'text': ' and dividing by the total number of points.', 'tokens': [50364, 293, 26764, 538, 264, 3217, 1230, 295, 2793, 13, 50496], 'temperature': 0.0, 'avg_logprob': -0.05912239306440977, 'compression_ratio': 1.612781954887218, 'no_speech_prob': 0.0001159177118097432}, {'id': 57, 'seek': 24240, 'start': 245.04, 'end': 248.4, 'text': ' Remember, unsupervised learning is about modeling the world,', 'tokens': [50496, 5459, 11, 2693, 12879, 24420, 2539, 307, 466, 15983, 264, 1002, 11, 50664], 'temperature': 0.0, 'avg_logprob': -0.05912239306440977, 'compression_ratio': 1.612781954887218, 'no_speech_prob': 0.0001159177118097432}, {'id': 58, 'seek': 24240, 'start': 248.4, 'end': 251.44, 'text': ' so our algorithm will have two steps.', 'tokens': [50664, 370, 527, 9284, 486, 362, 732, 4439, 13, 50816], 'temperature': 0.0, 'avg_logprob': -0.05912239306440977, 'compression_ratio': 1.612781954887218, 'no_speech_prob': 0.0001159177118097432}, {'id': 59, 'seek': 24240, 'start': 251.44, 'end': 253.84, 'text': ' First, our AI will predict.', 'tokens': [50816, 2386, 11, 527, 7318, 486, 6069, 13, 50936], 'temperature': 0.0, 'avg_logprob': -0.05912239306440977, 'compression_ratio': 1.612781954887218, 'no_speech_prob': 0.0001159177118097432}, {'id': 60, 'seek': 24240, 'start': 253.84, 'end': 256.48, 'text': ' What does the model expect the world to look like?', 'tokens': [50936, 708, 775, 264, 2316, 2066, 264, 1002, 281, 574, 411, 30, 51068], 'temperature': 0.0, 'avg_logprob': -0.05912239306440977, 'compression_ratio': 1.612781954887218, 'no_speech_prob': 0.0001159177118097432}, {'id': 61, 'seek': 24240, 'start': 256.48, 'end': 261.6, 'text': \" In other words, which flowers should be clustered together because they're the same species?\", 'tokens': [51068, 682, 661, 2283, 11, 597, 8085, 820, 312, 596, 38624, 1214, 570, 436, 434, 264, 912, 6172, 30, 51324], 'temperature': 0.0, 'avg_logprob': -0.05912239306440977, 'compression_ratio': 1.612781954887218, 'no_speech_prob': 0.0001159177118097432}, {'id': 62, 'seek': 24240, 'start': 261.6, 'end': 264.8, 'text': ' Second, our AI will correct or learn.', 'tokens': [51324, 5736, 11, 527, 7318, 486, 3006, 420, 1466, 13, 51484], 'temperature': 0.0, 'avg_logprob': -0.05912239306440977, 'compression_ratio': 1.612781954887218, 'no_speech_prob': 0.0001159177118097432}, {'id': 63, 'seek': 24240, 'start': 264.8, 'end': 269.04, 'text': ' The model will update its belief to agree with its observation of the world.', 'tokens': [51484, 440, 2316, 486, 5623, 1080, 7107, 281, 3986, 365, 1080, 14816, 295, 264, 1002, 13, 51696], 'temperature': 0.0, 'avg_logprob': -0.05912239306440977, 'compression_ratio': 1.612781954887218, 'no_speech_prob': 0.0001159177118097432}, {'id': 64, 'seek': 26904, 'start': 269.04, 'end': 273.84000000000003, 'text': ' To start the process, we have to specify how many clusters the model should look for.', 'tokens': [50364, 1407, 722, 264, 1399, 11, 321, 362, 281, 16500, 577, 867, 23313, 264, 2316, 820, 574, 337, 13, 50604], 'temperature': 0.0, 'avg_logprob': -0.07619120493656446, 'compression_ratio': 1.6148409893992932, 'no_speech_prob': 0.0004582964756991714}, {'id': 65, 'seek': 26904, 'start': 273.84000000000003, 'end': 276.48, 'text': \" I'm guessing there are three clusters in the data,\", 'tokens': [50604, 286, 478, 17939, 456, 366, 1045, 23313, 294, 264, 1412, 11, 50736], 'temperature': 0.0, 'avg_logprob': -0.07619120493656446, 'compression_ratio': 1.6148409893992932, 'no_speech_prob': 0.0004582964756991714}, {'id': 66, 'seek': 26904, 'start': 276.48, 'end': 280.0, 'text': \" so that becomes the model's initial understanding of the world.\", 'tokens': [50736, 370, 300, 3643, 264, 2316, 311, 5883, 3701, 295, 264, 1002, 13, 50912], 'temperature': 0.0, 'avg_logprob': -0.07619120493656446, 'compression_ratio': 1.6148409893992932, 'no_speech_prob': 0.0004582964756991714}, {'id': 67, 'seek': 26904, 'start': 280.0, 'end': 285.28000000000003, 'text': \" And we're looking for K equals 3 averages or 3 types of viruses.\", 'tokens': [50912, 400, 321, 434, 1237, 337, 591, 6915, 805, 42257, 420, 805, 3467, 295, 21785, 13, 51176], 'temperature': 0.0, 'avg_logprob': -0.07619120493656446, 'compression_ratio': 1.6148409893992932, 'no_speech_prob': 0.0004582964756991714}, {'id': 68, 'seek': 26904, 'start': 285.28000000000003, 'end': 288.56, 'text': \" But to start, our model doesn't really know anything.\", 'tokens': [51176, 583, 281, 722, 11, 527, 2316, 1177, 380, 534, 458, 1340, 13, 51340], 'temperature': 0.0, 'avg_logprob': -0.07619120493656446, 'compression_ratio': 1.6148409893992932, 'no_speech_prob': 0.0004582964756991714}, {'id': 69, 'seek': 26904, 'start': 288.56, 'end': 292.32000000000005, 'text': ' So the averages are random and so are its predictions.', 'tokens': [51340, 407, 264, 42257, 366, 4974, 293, 370, 366, 1080, 21264, 13, 51528], 'temperature': 0.0, 'avg_logprob': -0.07619120493656446, 'compression_ratio': 1.6148409893992932, 'no_speech_prob': 0.0004582964756991714}, {'id': 70, 'seek': 26904, 'start': 292.32000000000005, 'end': 298.96000000000004, 'text': ' Each data point, which is a flower, is given a label as type 1, type 2, or type 3.', 'tokens': [51528, 6947, 1412, 935, 11, 597, 307, 257, 8617, 11, 307, 2212, 257, 7645, 382, 2010, 502, 11, 2010, 568, 11, 420, 2010, 805, 13, 51860], 'temperature': 0.0, 'avg_logprob': -0.07619120493656446, 'compression_ratio': 1.6148409893992932, 'no_speech_prob': 0.0004582964756991714}, {'id': 71, 'seek': 29896, 'start': 299.03999999999996, 'end': 301.03999999999996, 'text': \" Based on the algorithm's beliefs.\", 'tokens': [50368, 18785, 322, 264, 9284, 311, 13585, 13, 50468], 'temperature': 0.0, 'avg_logprob': -0.07326204268658748, 'compression_ratio': 1.6366782006920415, 'no_speech_prob': 0.00015355832874774933}, {'id': 72, 'seek': 29896, 'start': 301.03999999999996, 'end': 303.52, 'text': ' Next, our model tries to correct itself.', 'tokens': [50468, 3087, 11, 527, 2316, 9898, 281, 3006, 2564, 13, 50592], 'temperature': 0.0, 'avg_logprob': -0.07326204268658748, 'compression_ratio': 1.6366782006920415, 'no_speech_prob': 0.00015355832874774933}, {'id': 73, 'seek': 29896, 'start': 303.52, 'end': 307.44, 'text': ' The average of each cluster of data points should be in the middle.', 'tokens': [50592, 440, 4274, 295, 1184, 13630, 295, 1412, 2793, 820, 312, 294, 264, 2808, 13, 50788], 'temperature': 0.0, 'avg_logprob': -0.07326204268658748, 'compression_ratio': 1.6366782006920415, 'no_speech_prob': 0.00015355832874774933}, {'id': 74, 'seek': 29896, 'start': 307.44, 'end': 311.03999999999996, 'text': ' So the model corrects itself by calculating new averages.', 'tokens': [50788, 407, 264, 2316, 3006, 82, 2564, 538, 28258, 777, 42257, 13, 50968], 'temperature': 0.0, 'avg_logprob': -0.07326204268658748, 'compression_ratio': 1.6366782006920415, 'no_speech_prob': 0.00015355832874774933}, {'id': 75, 'seek': 29896, 'start': 311.03999999999996, 'end': 313.76, 'text': \" We can see those averages here, marked with X's,\", 'tokens': [50968, 492, 393, 536, 729, 42257, 510, 11, 12658, 365, 1783, 311, 11, 51104], 'temperature': 0.0, 'avg_logprob': -0.07326204268658748, 'compression_ratio': 1.6366782006920415, 'no_speech_prob': 0.00015355832874774933}, {'id': 76, 'seek': 29896, 'start': 313.76, 'end': 319.2, 'text': ' which gives us an updated model of the three, or so we guessed, types of viruses.', 'tokens': [51104, 597, 2709, 505, 364, 10588, 2316, 295, 264, 1045, 11, 420, 370, 321, 21852, 11, 3467, 295, 21785, 13, 51376], 'temperature': 0.0, 'avg_logprob': -0.07326204268658748, 'compression_ratio': 1.6366782006920415, 'no_speech_prob': 0.00015355832874774933}, {'id': 77, 'seek': 29896, 'start': 319.2, 'end': 322.24, 'text': ' But the graph is still pretty noisy.', 'tokens': [51376, 583, 264, 4295, 307, 920, 1238, 24518, 13, 51528], 'temperature': 0.0, 'avg_logprob': -0.07326204268658748, 'compression_ratio': 1.6366782006920415, 'no_speech_prob': 0.00015355832874774933}, {'id': 78, 'seek': 29896, 'start': 322.24, 'end': 327.76, 'text': \" For example, it's a little weird that we have type 2 flowers so close to the average for type 3 flowers.\", 'tokens': [51528, 1171, 1365, 11, 309, 311, 257, 707, 3657, 300, 321, 362, 2010, 568, 8085, 370, 1998, 281, 264, 4274, 337, 2010, 805, 8085, 13, 51804], 'temperature': 0.0, 'avg_logprob': -0.07326204268658748, 'compression_ratio': 1.6366782006920415, 'no_speech_prob': 0.00015355832874774933}, {'id': 79, 'seek': 32776, 'start': 327.84, 'end': 333.12, 'text': \" But we did start with a random model, so we can't expect too much accuracy.\", 'tokens': [50368, 583, 321, 630, 722, 365, 257, 4974, 2316, 11, 370, 321, 393, 380, 2066, 886, 709, 14170, 13, 50632], 'temperature': 0.0, 'avg_logprob': -0.0666666030883789, 'compression_ratio': 1.6766917293233083, 'no_speech_prob': 0.00010229821054963395}, {'id': 80, 'seek': 32776, 'start': 333.12, 'end': 338.48, 'text': ' Logically, we know that irises of the same species tend to have similar petals.', 'tokens': [50632, 10824, 984, 11, 321, 458, 300, 3418, 3598, 295, 264, 912, 6172, 3928, 281, 362, 2531, 31530, 13, 50900], 'temperature': 0.0, 'avg_logprob': -0.0666666030883789, 'compression_ratio': 1.6766917293233083, 'no_speech_prob': 0.00010229821054963395}, {'id': 81, 'seek': 32776, 'start': 338.48, 'end': 341.28, 'text': ' So those data points should be clustered together.', 'tokens': [50900, 407, 729, 1412, 2793, 820, 312, 596, 38624, 1214, 13, 51040], 'temperature': 0.0, 'avg_logprob': -0.0666666030883789, 'compression_ratio': 1.6766917293233083, 'no_speech_prob': 0.00010229821054963395}, {'id': 82, 'seek': 32776, 'start': 341.92, 'end': 344.71999999999997, 'text': ' Since we did a correction or learning step,', 'tokens': [51072, 4162, 321, 630, 257, 19984, 420, 2539, 1823, 11, 51212], 'temperature': 0.0, 'avg_logprob': -0.0666666030883789, 'compression_ratio': 1.6766917293233083, 'no_speech_prob': 0.00010229821054963395}, {'id': 83, 'seek': 32776, 'start': 344.71999999999997, 'end': 348.32, 'text': ' we can repeat the process, starting with a new prediction step.', 'tokens': [51212, 321, 393, 7149, 264, 1399, 11, 2891, 365, 257, 777, 17630, 1823, 13, 51392], 'temperature': 0.0, 'avg_logprob': -0.0666666030883789, 'compression_ratio': 1.6766917293233083, 'no_speech_prob': 0.00010229821054963395}, {'id': 84, 'seek': 32776, 'start': 348.32, 'end': 353.52, 'text': \" Let's predict new labels using the X's that mark the averages of each label.\", 'tokens': [51392, 961, 311, 6069, 777, 16949, 1228, 264, 1783, 311, 300, 1491, 264, 42257, 295, 1184, 7645, 13, 51652], 'temperature': 0.0, 'avg_logprob': -0.0666666030883789, 'compression_ratio': 1.6766917293233083, 'no_speech_prob': 0.00010229821054963395}, {'id': 85, 'seek': 32776, 'start': 353.52, 'end': 356.71999999999997, 'text': \" We'll give each data point the label of its closest X.\", 'tokens': [51652, 492, 603, 976, 1184, 1412, 935, 264, 7645, 295, 1080, 13699, 1783, 13, 51812], 'temperature': 0.0, 'avg_logprob': -0.0666666030883789, 'compression_ratio': 1.6766917293233083, 'no_speech_prob': 0.00010229821054963395}, {'id': 86, 'seek': 35672, 'start': 356.72, 'end': 359.36, 'text': ' Type 1, type 2, or type 3.', 'tokens': [50364, 15576, 502, 11, 2010, 568, 11, 420, 2010, 805, 13, 50496], 'temperature': 0.0, 'avg_logprob': -0.09223074016004505, 'compression_ratio': 1.5158371040723981, 'no_speech_prob': 0.0001233911025337875}, {'id': 87, 'seek': 35672, 'start': 359.36, 'end': 362.32000000000005, 'text': \" And then we'll calculate new averages.\", 'tokens': [50496, 400, 550, 321, 603, 8873, 777, 42257, 13, 50644], 'temperature': 0.0, 'avg_logprob': -0.09223074016004505, 'compression_ratio': 1.5158371040723981, 'no_speech_prob': 0.0001233911025337875}, {'id': 88, 'seek': 35672, 'start': 362.32000000000005, 'end': 364.48, 'text': \" Ah, that's better.\", 'tokens': [50644, 2438, 11, 300, 311, 1101, 13, 50752], 'temperature': 0.0, 'avg_logprob': -0.09223074016004505, 'compression_ratio': 1.5158371040723981, 'no_speech_prob': 0.0001233911025337875}, {'id': 89, 'seek': 35672, 'start': 364.48, 'end': 367.84000000000003, 'text': ' But still not the cleanest clusters.', 'tokens': [50752, 583, 920, 406, 264, 2541, 377, 23313, 13, 50920], 'temperature': 0.0, 'avg_logprob': -0.09223074016004505, 'compression_ratio': 1.5158371040723981, 'no_speech_prob': 0.0001233911025337875}, {'id': 90, 'seek': 35672, 'start': 367.84000000000003, 'end': 370.08000000000004, 'text': ' So we can repeat the process again.', 'tokens': [50920, 407, 321, 393, 7149, 264, 1399, 797, 13, 51032], 'temperature': 0.0, 'avg_logprob': -0.09223074016004505, 'compression_ratio': 1.5158371040723981, 'no_speech_prob': 0.0001233911025337875}, {'id': 91, 'seek': 35672, 'start': 370.08000000000004, 'end': 373.36, 'text': ' Predict, learn, predict, learn.', 'tokens': [51032, 430, 24945, 11, 1466, 11, 6069, 11, 1466, 13, 51196], 'temperature': 0.0, 'avg_logprob': -0.09223074016004505, 'compression_ratio': 1.5158371040723981, 'no_speech_prob': 0.0001233911025337875}, {'id': 92, 'seek': 35672, 'start': 373.36, 'end': 378.32000000000005, 'text': \" Eventually, the X's will stop moving and we will have a model of iris clusters created\", 'tokens': [51196, 17586, 11, 264, 1783, 311, 486, 1590, 2684, 293, 321, 486, 362, 257, 2316, 295, 3418, 271, 23313, 2942, 51444], 'temperature': 0.0, 'avg_logprob': -0.09223074016004505, 'compression_ratio': 1.5158371040723981, 'no_speech_prob': 0.0001233911025337875}, {'id': 93, 'seek': 35672, 'start': 378.32000000000005, 'end': 380.56, 'text': ' with unsupervised learning.', 'tokens': [51444, 365, 2693, 12879, 24420, 2539, 13, 51556], 'temperature': 0.0, 'avg_logprob': -0.09223074016004505, 'compression_ratio': 1.5158371040723981, 'no_speech_prob': 0.0001233911025337875}, {'id': 94, 'seek': 35672, 'start': 380.56, 'end': 382.96000000000004, 'text': ' Now, the ultimate question is,', 'tokens': [51556, 823, 11, 264, 9705, 1168, 307, 11, 51676], 'temperature': 0.0, 'avg_logprob': -0.09223074016004505, 'compression_ratio': 1.5158371040723981, 'no_speech_prob': 0.0001233911025337875}, {'id': 95, 'seek': 38296, 'start': 382.96, 'end': 386.79999999999995, 'text': ' did we find meaningful patterns about the world with our AI?', 'tokens': [50364, 630, 321, 915, 10995, 8294, 466, 264, 1002, 365, 527, 7318, 30, 50556], 'temperature': 0.0, 'avg_logprob': -0.07312658454189781, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 0.0018099696608260274}, {'id': 96, 'seek': 38296, 'start': 386.79999999999995, 'end': 390.0, 'text': ' We made an assumption that there were three types of irises', 'tokens': [50556, 492, 1027, 364, 15302, 300, 456, 645, 1045, 3467, 295, 3418, 3598, 50716], 'temperature': 0.0, 'avg_logprob': -0.07312658454189781, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 0.0018099696608260274}, {'id': 97, 'seek': 38296, 'start': 390.0, 'end': 392.96, 'text': ' and we assumed that they had different petal lengths and widths.', 'tokens': [50716, 293, 321, 15895, 300, 436, 632, 819, 3817, 304, 26329, 293, 11402, 82, 13, 50864], 'temperature': 0.0, 'avg_logprob': -0.07312658454189781, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 0.0018099696608260274}, {'id': 98, 'seek': 38296, 'start': 393.76, 'end': 394.71999999999997, 'text': ' Was this true?', 'tokens': [50904, 3027, 341, 2074, 30, 50952], 'temperature': 0.0, 'avg_logprob': -0.07312658454189781, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 0.0018099696608260274}, {'id': 99, 'seek': 38296, 'start': 394.71999999999997, 'end': 398.47999999999996, 'text': ' Lucky for us, I have a friend who is a master gardener.', 'tokens': [50952, 26639, 337, 505, 11, 286, 362, 257, 1277, 567, 307, 257, 4505, 7431, 260, 13, 51140], 'temperature': 0.0, 'avg_logprob': -0.07312658454189781, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 0.0018099696608260274}, {'id': 100, 'seek': 38296, 'start': 398.47999999999996, 'end': 402.47999999999996, 'text': ' I showed him the real life flowers closest to each of the three averages', 'tokens': [51140, 286, 4712, 796, 264, 957, 993, 8085, 13699, 281, 1184, 295, 264, 1045, 42257, 51340], 'temperature': 0.0, 'avg_logprob': -0.07312658454189781, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 0.0018099696608260274}, {'id': 101, 'seek': 38296, 'start': 402.47999999999996, 'end': 409.28, 'text': ' and he said that type 1 is versicolor, type 2 is satosa, and type 3 is virginica.', 'tokens': [51340, 293, 415, 848, 300, 2010, 502, 307, 1774, 299, 36182, 11, 2010, 568, 307, 3227, 6447, 11, 293, 2010, 805, 307, 26404, 2262, 13, 51680], 'temperature': 0.0, 'avg_logprob': -0.07312658454189781, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 0.0018099696608260274}, {'id': 102, 'seek': 38296, 'start': 409.28, 'end': 411.84, 'text': ' Three different iris species.', 'tokens': [51680, 6244, 819, 3418, 271, 6172, 13, 51808], 'temperature': 0.0, 'avg_logprob': -0.07312658454189781, 'compression_ratio': 1.651685393258427, 'no_speech_prob': 0.0018099696608260274}, {'id': 103, 'seek': 41184, 'start': 411.84, 'end': 417.44, 'text': ' We learned about the world from observation, which is what makes this unsupervised learning.', 'tokens': [50364, 492, 3264, 466, 264, 1002, 490, 14816, 11, 597, 307, 437, 1669, 341, 2693, 12879, 24420, 2539, 13, 50644], 'temperature': 0.0, 'avg_logprob': -0.0600281042211196, 'compression_ratio': 1.6620209059233448, 'no_speech_prob': 5.6496955949114636e-05}, {'id': 104, 'seek': 41184, 'start': 417.44, 'end': 423.35999999999996, 'text': ' Even though we relied a tiny bit on a teacher, the master gardener, for confirmation and help.', 'tokens': [50644, 2754, 1673, 321, 35463, 257, 5870, 857, 322, 257, 5027, 11, 264, 4505, 7431, 260, 11, 337, 21871, 293, 854, 13, 50940], 'temperature': 0.0, 'avg_logprob': -0.0600281042211196, 'compression_ratio': 1.6620209059233448, 'no_speech_prob': 5.6496955949114636e-05}, {'id': 105, 'seek': 41184, 'start': 423.35999999999996, 'end': 427.52, 'text': \" Now that we've learned the basics, we can experiment with harder examples.\", 'tokens': [50940, 823, 300, 321, 600, 3264, 264, 14688, 11, 321, 393, 5120, 365, 6081, 5110, 13, 51148], 'temperature': 0.0, 'avg_logprob': -0.0600281042211196, 'compression_ratio': 1.6620209059233448, 'no_speech_prob': 5.6496955949114636e-05}, {'id': 106, 'seek': 41184, 'start': 427.52, 'end': 431.44, 'text': \" Let's say we want to use unsupervised learning to sort a bunch of different photos,\", 'tokens': [51148, 961, 311, 584, 321, 528, 281, 764, 2693, 12879, 24420, 2539, 281, 1333, 257, 3840, 295, 819, 5787, 11, 51344], 'temperature': 0.0, 'avg_logprob': -0.0600281042211196, 'compression_ratio': 1.6620209059233448, 'no_speech_prob': 5.6496955949114636e-05}, {'id': 107, 'seek': 41184, 'start': 431.44, 'end': 433.67999999999995, 'text': ' and not just three iris species.', 'tokens': [51344, 293, 406, 445, 1045, 3418, 271, 6172, 13, 51456], 'temperature': 0.0, 'avg_logprob': -0.0600281042211196, 'compression_ratio': 1.6620209059233448, 'no_speech_prob': 5.6496955949114636e-05}, {'id': 108, 'seek': 41184, 'start': 433.67999999999995, 'end': 436.47999999999996, 'text': ' First, what observations can we measure?', 'tokens': [51456, 2386, 11, 437, 18163, 393, 321, 3481, 30, 51596], 'temperature': 0.0, 'avg_logprob': -0.0600281042211196, 'compression_ratio': 1.6620209059233448, 'no_speech_prob': 5.6496955949114636e-05}, {'id': 109, 'seek': 41184, 'start': 436.47999999999996, 'end': 439.52, 'text': \" How much green there is, whether there's a nose and fur?\", 'tokens': [51596, 1012, 709, 3092, 456, 307, 11, 1968, 456, 311, 257, 6690, 293, 2687, 30, 51748], 'temperature': 0.0, 'avg_logprob': -0.0600281042211196, 'compression_ratio': 1.6620209059233448, 'no_speech_prob': 5.6496955949114636e-05}, {'id': 110, 'seek': 43952, 'start': 439.52, 'end': 443.76, 'text': ' To have a computer make these observations, we need to measure thousands of red, green,', 'tokens': [50364, 1407, 362, 257, 3820, 652, 613, 18163, 11, 321, 643, 281, 3481, 5383, 295, 2182, 11, 3092, 11, 50576], 'temperature': 0.0, 'avg_logprob': -0.08614450588560942, 'compression_ratio': 1.6821428571428572, 'no_speech_prob': 0.0002531491918489337}, {'id': 111, 'seek': 43952, 'start': 443.76, 'end': 445.91999999999996, 'text': ' and blue pixels in each image.', 'tokens': [50576, 293, 3344, 18668, 294, 1184, 3256, 13, 50684], 'temperature': 0.0, 'avg_logprob': -0.08614450588560942, 'compression_ratio': 1.6821428571428572, 'no_speech_prob': 0.0002531491918489337}, {'id': 112, 'seek': 43952, 'start': 445.91999999999996, 'end': 448.64, 'text': ' Second, how do we want to represent the world?', 'tokens': [50684, 5736, 11, 577, 360, 321, 528, 281, 2906, 264, 1002, 30, 50820], 'temperature': 0.0, 'avg_logprob': -0.08614450588560942, 'compression_ratio': 1.6821428571428572, 'no_speech_prob': 0.0002531491918489337}, {'id': 113, 'seek': 43952, 'start': 448.64, 'end': 453.44, 'text': ' Before, we were only working with two features, so we could just use the averages of the cluster', 'tokens': [50820, 4546, 11, 321, 645, 787, 1364, 365, 732, 4122, 11, 370, 321, 727, 445, 764, 264, 42257, 295, 264, 13630, 51060], 'temperature': 0.0, 'avg_logprob': -0.08614450588560942, 'compression_ratio': 1.6821428571428572, 'no_speech_prob': 0.0002531491918489337}, {'id': 114, 'seek': 43952, 'start': 453.44, 'end': 456.08, 'text': ' data points and get meaningful extraction from it.', 'tokens': [51060, 1412, 2793, 293, 483, 10995, 30197, 490, 309, 13, 51192], 'temperature': 0.0, 'avg_logprob': -0.08614450588560942, 'compression_ratio': 1.6821428571428572, 'no_speech_prob': 0.0002531491918489337}, {'id': 115, 'seek': 43952, 'start': 456.08, 'end': 459.84, 'text': \" But when dealing with images, we can't use the same method,\", 'tokens': [51192, 583, 562, 6260, 365, 5267, 11, 321, 393, 380, 764, 264, 912, 3170, 11, 51380], 'temperature': 0.0, 'avg_logprob': -0.08614450588560942, 'compression_ratio': 1.6821428571428572, 'no_speech_prob': 0.0002531491918489337}, {'id': 116, 'seek': 43952, 'start': 459.84, 'end': 465.35999999999996, 'text': \" because we won't get much meaning out of averaging colored pixels for what we want to accomplish.\", 'tokens': [51380, 570, 321, 1582, 380, 483, 709, 3620, 484, 295, 47308, 14332, 18668, 337, 437, 321, 528, 281, 9021, 13, 51656], 'temperature': 0.0, 'avg_logprob': -0.08614450588560942, 'compression_ratio': 1.6821428571428572, 'no_speech_prob': 0.0002531491918489337}, {'id': 117, 'seek': 46536, 'start': 465.44, 'end': 471.6, 'text': ' Somehow, we need the model to create a representation that tells us if two images are similar.', 'tokens': [50368, 28357, 11, 321, 643, 264, 2316, 281, 1884, 257, 10290, 300, 5112, 505, 498, 732, 5267, 366, 2531, 13, 50676], 'temperature': 0.0, 'avg_logprob': -0.07011369000310483, 'compression_ratio': 1.8333333333333333, 'no_speech_prob': 0.00012339161185082048}, {'id': 118, 'seek': 46536, 'start': 471.6, 'end': 476.48, 'text': ' There are meaningful patterns in the data that are more abstract than individual pixels,', 'tokens': [50676, 821, 366, 10995, 8294, 294, 264, 1412, 300, 366, 544, 12649, 813, 2609, 18668, 11, 50920], 'temperature': 0.0, 'avg_logprob': -0.07011369000310483, 'compression_ratio': 1.8333333333333333, 'no_speech_prob': 0.00012339161185082048}, {'id': 119, 'seek': 46536, 'start': 476.48, 'end': 481.68, 'text': \" and finding them across many images is what's called representation learning.\", 'tokens': [50920, 293, 5006, 552, 2108, 867, 5267, 307, 437, 311, 1219, 10290, 2539, 13, 51180], 'temperature': 0.0, 'avg_logprob': -0.07011369000310483, 'compression_ratio': 1.8333333333333333, 'no_speech_prob': 0.00012339161185082048}, {'id': 120, 'seek': 46536, 'start': 481.68, 'end': 486.48, 'text': \" These patterns help us understand what's in the images and how to compare them to each other.\", 'tokens': [51180, 1981, 8294, 854, 505, 1223, 437, 311, 294, 264, 5267, 293, 577, 281, 6794, 552, 281, 1184, 661, 13, 51420], 'temperature': 0.0, 'avg_logprob': -0.07011369000310483, 'compression_ratio': 1.8333333333333333, 'no_speech_prob': 0.00012339161185082048}, {'id': 121, 'seek': 46536, 'start': 486.48, 'end': 491.36, 'text': ' Representation learning happens both in supervised and unsupervised learning models,', 'tokens': [51420, 19945, 399, 2539, 2314, 1293, 294, 46533, 293, 2693, 12879, 24420, 2539, 5245, 11, 51664], 'temperature': 0.0, 'avg_logprob': -0.07011369000310483, 'compression_ratio': 1.8333333333333333, 'no_speech_prob': 0.00012339161185082048}, {'id': 122, 'seek': 49136, 'start': 491.36, 'end': 495.6, 'text': ' so we can do it with or without labels to find patterns in the world.', 'tokens': [50364, 370, 321, 393, 360, 309, 365, 420, 1553, 16949, 281, 915, 8294, 294, 264, 1002, 13, 50576], 'temperature': 0.0, 'avg_logprob': -0.08467115853962145, 'compression_ratio': 1.4975124378109452, 'no_speech_prob': 0.0001911007857415825}, {'id': 123, 'seek': 49136, 'start': 495.6, 'end': 500.24, 'text': ' To understand the basic idea of representation learning, check out this experiment.', 'tokens': [50576, 1407, 1223, 264, 3875, 1558, 295, 10290, 2539, 11, 1520, 484, 341, 5120, 13, 50808], 'temperature': 0.0, 'avg_logprob': -0.08467115853962145, 'compression_ratio': 1.4975124378109452, 'no_speech_prob': 0.0001911007857415825}, {'id': 124, 'seek': 49136, 'start': 500.24, 'end': 503.6, 'text': \" I'm going to look at a picture really fast and then try and draw it.\", 'tokens': [50808, 286, 478, 516, 281, 574, 412, 257, 3036, 534, 2370, 293, 550, 853, 293, 2642, 309, 13, 50976], 'temperature': 0.0, 'avg_logprob': -0.08467115853962145, 'compression_ratio': 1.4975124378109452, 'no_speech_prob': 0.0001911007857415825}, {'id': 125, 'seek': 49136, 'start': 503.6, 'end': 505.44, 'text': ' Ready, set, go.', 'tokens': [50976, 9944, 11, 992, 11, 352, 13, 51068], 'temperature': 0.0, 'avg_logprob': -0.08467115853962145, 'compression_ratio': 1.4975124378109452, 'no_speech_prob': 0.0001911007857415825}, {'id': 126, 'seek': 49136, 'start': 514.96, 'end': 518.72, 'text': ' My eyes took in the picture and remembered important features,', 'tokens': [51544, 1222, 2575, 1890, 294, 264, 3036, 293, 13745, 1021, 4122, 11, 51732], 'temperature': 0.0, 'avg_logprob': -0.08467115853962145, 'compression_ratio': 1.4975124378109452, 'no_speech_prob': 0.0001911007857415825}, {'id': 127, 'seek': 51872, 'start': 518.72, 'end': 521.84, 'text': \" so I'm building a representation in my mind.\", 'tokens': [50364, 370, 286, 478, 2390, 257, 10290, 294, 452, 1575, 13, 50520], 'temperature': 0.0, 'avg_logprob': -0.08976821899414063, 'compression_ratio': 1.5076142131979695, 'no_speech_prob': 9.914935799315572e-05}, {'id': 128, 'seek': 51872, 'start': 521.84, 'end': 526.5600000000001, 'text': \" But I can't just show you my thoughts to get feedback on what parts I misremembered,\", 'tokens': [50520, 583, 286, 393, 380, 445, 855, 291, 452, 4598, 281, 483, 5824, 322, 437, 3166, 286, 3346, 2579, 1304, 292, 11, 50756], 'temperature': 0.0, 'avg_logprob': -0.08976821899414063, 'compression_ratio': 1.5076142131979695, 'no_speech_prob': 9.914935799315572e-05}, {'id': 129, 'seek': 51872, 'start': 526.5600000000001, 'end': 531.9200000000001, 'text': ' so I have to produce a reconstruction or draw the original image from my memory.', 'tokens': [50756, 370, 286, 362, 281, 5258, 257, 31565, 420, 2642, 264, 3380, 3256, 490, 452, 4675, 13, 51024], 'temperature': 0.0, 'avg_logprob': -0.08976821899414063, 'compression_ratio': 1.5076142131979695, 'no_speech_prob': 9.914935799315572e-05}, {'id': 130, 'seek': 51872, 'start': 540.24, 'end': 542.8000000000001, 'text': \" Alright, so this is what I've got.\", 'tokens': [51440, 2798, 11, 370, 341, 307, 437, 286, 600, 658, 13, 51568], 'temperature': 0.0, 'avg_logprob': -0.08976821899414063, 'compression_ratio': 1.5076142131979695, 'no_speech_prob': 9.914935799315572e-05}, {'id': 131, 'seek': 51872, 'start': 542.8000000000001, 'end': 546.32, 'text': \" Now let's compare my drawing to the original image.\", 'tokens': [51568, 823, 718, 311, 6794, 452, 6316, 281, 264, 3380, 3256, 13, 51744], 'temperature': 0.0, 'avg_logprob': -0.08976821899414063, 'compression_ratio': 1.5076142131979695, 'no_speech_prob': 9.914935799315572e-05}, {'id': 132, 'seek': 54632, 'start': 546.32, 'end': 553.6, 'text': \" Let's see, round plate, triangle slice of pizza, some cheese, some crust, tablecloth.\", 'tokens': [50364, 961, 311, 536, 11, 3098, 5924, 11, 13369, 13153, 295, 8298, 11, 512, 5399, 11, 512, 18156, 11, 3199, 3474, 900, 13, 50728], 'temperature': 0.0, 'avg_logprob': -0.12135019107740752, 'compression_ratio': 1.6124031007751938, 'no_speech_prob': 0.00015843169239815325}, {'id': 133, 'seek': 54632, 'start': 553.6, 'end': 554.24, 'text': ' Pretty good.', 'tokens': [50728, 10693, 665, 13, 50760], 'temperature': 0.0, 'avg_logprob': -0.12135019107740752, 'compression_ratio': 1.6124031007751938, 'no_speech_prob': 0.00015843169239815325}, {'id': 134, 'seek': 54632, 'start': 554.24, 'end': 559.12, 'text': ' For an AI, making a reconstruction would mean producing all the right pixel values to make', 'tokens': [50760, 1171, 364, 7318, 11, 1455, 257, 31565, 576, 914, 10501, 439, 264, 558, 19261, 4190, 281, 652, 51004], 'temperature': 0.0, 'avg_logprob': -0.12135019107740752, 'compression_ratio': 1.6124031007751938, 'no_speech_prob': 0.00015843169239815325}, {'id': 135, 'seek': 54632, 'start': 559.12, 'end': 560.32, 'text': ' a reconstruction.', 'tokens': [51004, 257, 31565, 13, 51064], 'temperature': 0.0, 'avg_logprob': -0.12135019107740752, 'compression_ratio': 1.6124031007751938, 'no_speech_prob': 0.00015843169239815325}, {'id': 136, 'seek': 54632, 'start': 560.32, 'end': 564.96, 'text': ' Arcane means clustering algorithm from before predicted classes for flowers based on how', 'tokens': [51064, 21727, 1929, 1355, 596, 48673, 9284, 490, 949, 19147, 5359, 337, 8085, 2361, 322, 577, 51296], 'temperature': 0.0, 'avg_logprob': -0.12135019107740752, 'compression_ratio': 1.6124031007751938, 'no_speech_prob': 0.00015843169239815325}, {'id': 137, 'seek': 54632, 'start': 564.96, 'end': 567.6800000000001, 'text': ' close the data points were to the averages.', 'tokens': [51296, 1998, 264, 1412, 2793, 645, 281, 264, 42257, 13, 51432], 'temperature': 0.0, 'avg_logprob': -0.12135019107740752, 'compression_ratio': 1.6124031007751938, 'no_speech_prob': 0.00015843169239815325}, {'id': 138, 'seek': 54632, 'start': 567.6800000000001, 'end': 572.1600000000001, 'text': ' For images, we will have learned image representations instead of averages.', 'tokens': [51432, 1171, 5267, 11, 321, 486, 362, 3264, 3256, 33358, 2602, 295, 42257, 13, 51656], 'temperature': 0.0, 'avg_logprob': -0.12135019107740752, 'compression_ratio': 1.6124031007751938, 'no_speech_prob': 0.00015843169239815325}, {'id': 139, 'seek': 57216, 'start': 572.16, 'end': 576.88, 'text': ' After that step, just like before, the AI will have to correct itself.', 'tokens': [50364, 2381, 300, 1823, 11, 445, 411, 949, 11, 264, 7318, 486, 362, 281, 3006, 2564, 13, 50600], 'temperature': 0.0, 'avg_logprob': -0.07948816264117206, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 8.480995165882632e-05}, {'id': 140, 'seek': 57216, 'start': 576.88, 'end': 583.04, 'text': ' Previously, we updated the K clusters based on how well our predicted labels fit the data,', 'tokens': [50600, 33606, 11, 321, 10588, 264, 591, 23313, 2361, 322, 577, 731, 527, 19147, 16949, 3318, 264, 1412, 11, 50908], 'temperature': 0.0, 'avg_logprob': -0.07948816264117206, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 8.480995165882632e-05}, {'id': 141, 'seek': 57216, 'start': 583.04, 'end': 589.76, 'text': \" but for images, we'd have to update the model's internal representations based on its reconstructions.\", 'tokens': [50908, 457, 337, 5267, 11, 321, 1116, 362, 281, 5623, 264, 2316, 311, 6920, 33358, 2361, 322, 1080, 31499, 626, 13, 51244], 'temperature': 0.0, 'avg_logprob': -0.07948816264117206, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 8.480995165882632e-05}, {'id': 142, 'seek': 57216, 'start': 589.76, 'end': 594.24, 'text': ' There are different ways to use unsupervised learning in combination with representation', 'tokens': [51244, 821, 366, 819, 2098, 281, 764, 2693, 12879, 24420, 2539, 294, 6562, 365, 10290, 51468], 'temperature': 0.0, 'avg_logprob': -0.07948816264117206, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 8.480995165882632e-05}, {'id': 143, 'seek': 57216, 'start': 594.24, 'end': 597.12, 'text': ' learning so that an AI can compare images.', 'tokens': [51468, 2539, 370, 300, 364, 7318, 393, 6794, 5267, 13, 51612], 'temperature': 0.0, 'avg_logprob': -0.07948816264117206, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 8.480995165882632e-05}, {'id': 144, 'seek': 57216, 'start': 597.12, 'end': 601.36, 'text': \" Like for example, there's a type of neural network called an autoencoder,\", 'tokens': [51612, 1743, 337, 1365, 11, 456, 311, 257, 2010, 295, 18161, 3209, 1219, 364, 8399, 22660, 19866, 11, 51824], 'temperature': 0.0, 'avg_logprob': -0.07948816264117206, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 8.480995165882632e-05}, {'id': 145, 'seek': 60136, 'start': 601.44, 'end': 606.0, 'text': ' which uses the same basic principles of weights and biases to process inputs,', 'tokens': [50368, 597, 4960, 264, 912, 3875, 9156, 295, 17443, 293, 32152, 281, 1399, 15743, 11, 50596], 'temperature': 0.0, 'avg_logprob': -0.06562752383095878, 'compression_ratio': 1.7865168539325842, 'no_speech_prob': 6.401902646757662e-05}, {'id': 146, 'seek': 60136, 'start': 606.0, 'end': 610.5600000000001, 'text': ' pass data onto the hidden layers, and finally to a prediction output layer.', 'tokens': [50596, 1320, 1412, 3911, 264, 7633, 7914, 11, 293, 2721, 281, 257, 17630, 5598, 4583, 13, 50824], 'temperature': 0.0, 'avg_logprob': -0.06562752383095878, 'compression_ratio': 1.7865168539325842, 'no_speech_prob': 6.401902646757662e-05}, {'id': 147, 'seek': 60136, 'start': 610.5600000000001, 'end': 615.04, 'text': ' If John Greenbott was programmed with an autoencoder, the input would be an image,', 'tokens': [50824, 759, 2619, 6969, 65, 1521, 390, 31092, 365, 364, 8399, 22660, 19866, 11, 264, 4846, 576, 312, 364, 3256, 11, 51048], 'temperature': 0.0, 'avg_logprob': -0.06562752383095878, 'compression_ratio': 1.7865168539325842, 'no_speech_prob': 6.401902646757662e-05}, {'id': 148, 'seek': 60136, 'start': 615.04, 'end': 620.48, 'text': ' the hidden layers would contain representations, and the output would be a full reconstruction', 'tokens': [51048, 264, 7633, 7914, 576, 5304, 33358, 11, 293, 264, 5598, 576, 312, 257, 1577, 31565, 51320], 'temperature': 0.0, 'avg_logprob': -0.06562752383095878, 'compression_ratio': 1.7865168539325842, 'no_speech_prob': 6.401902646757662e-05}, {'id': 149, 'seek': 60136, 'start': 620.48, 'end': 622.32, 'text': ' of the original image.', 'tokens': [51320, 295, 264, 3380, 3256, 13, 51412], 'temperature': 0.0, 'avg_logprob': -0.06562752383095878, 'compression_ratio': 1.7865168539325842, 'no_speech_prob': 6.401902646757662e-05}, {'id': 150, 'seek': 60136, 'start': 622.32, 'end': 625.04, 'text': ' Which gets more accurate the more we train this AI.', 'tokens': [51412, 3013, 2170, 544, 8559, 264, 544, 321, 3847, 341, 7318, 13, 51548], 'temperature': 0.0, 'avg_logprob': -0.06562752383095878, 'compression_ratio': 1.7865168539325842, 'no_speech_prob': 6.401902646757662e-05}, {'id': 151, 'seek': 60136, 'start': 625.04, 'end': 628.88, 'text': ' Theoretically, I could give John Greenbott a representation of a pizza', 'tokens': [51548, 440, 26262, 984, 11, 286, 727, 976, 2619, 6969, 65, 1521, 257, 10290, 295, 257, 8298, 51740], 'temperature': 0.0, 'avg_logprob': -0.06562752383095878, 'compression_ratio': 1.7865168539325842, 'no_speech_prob': 6.401902646757662e-05}, {'id': 152, 'seek': 62888, 'start': 628.88, 'end': 631.6, 'text': ' and he could reconstruct the original pizza image.', 'tokens': [50364, 293, 415, 727, 31499, 264, 3380, 8298, 3256, 13, 50500], 'temperature': 0.0, 'avg_logprob': -0.10986096628250615, 'compression_ratio': 1.6442953020134228, 'no_speech_prob': 0.0005702855996787548}, {'id': 153, 'seek': 62888, 'start': 631.6, 'end': 636.24, 'text': \" What's so powerful about unsupervised learning is that the world is our teacher.\", 'tokens': [50500, 708, 311, 370, 4005, 466, 2693, 12879, 24420, 2539, 307, 300, 264, 1002, 307, 527, 5027, 13, 50732], 'temperature': 0.0, 'avg_logprob': -0.10986096628250615, 'compression_ratio': 1.6442953020134228, 'no_speech_prob': 0.0005702855996787548}, {'id': 154, 'seek': 62888, 'start': 636.24, 'end': 640.56, 'text': \" By looking around, taking in a lot of data, and predicting what we'll see next,\", 'tokens': [50732, 3146, 1237, 926, 11, 1940, 294, 257, 688, 295, 1412, 11, 293, 32884, 437, 321, 603, 536, 958, 11, 50948], 'temperature': 0.0, 'avg_logprob': -0.10986096628250615, 'compression_ratio': 1.6442953020134228, 'no_speech_prob': 0.0005702855996787548}, {'id': 155, 'seek': 62888, 'start': 640.56, 'end': 644.56, 'text': ' we learn about how the world works and how it should be represented.', 'tokens': [50948, 321, 1466, 466, 577, 264, 1002, 1985, 293, 577, 309, 820, 312, 10379, 13, 51148], 'temperature': 0.0, 'avg_logprob': -0.10986096628250615, 'compression_ratio': 1.6442953020134228, 'no_speech_prob': 0.0005702855996787548}, {'id': 156, 'seek': 62888, 'start': 644.56, 'end': 647.52, 'text': ' When asked how AI will fulfill its grand ambitions,', 'tokens': [51148, 1133, 2351, 577, 7318, 486, 13875, 1080, 2697, 34475, 11, 51296], 'temperature': 0.0, 'avg_logprob': -0.10986096628250615, 'compression_ratio': 1.6442953020134228, 'no_speech_prob': 0.0005702855996787548}, {'id': 157, 'seek': 62888, 'start': 647.52, 'end': 651.36, 'text': ' 2018 Turing Award winning professor, Jan LeCun said,', 'tokens': [51296, 6096, 314, 1345, 13894, 8224, 8304, 11, 4956, 1456, 34, 409, 848, 11, 51488], 'temperature': 0.0, 'avg_logprob': -0.10986096628250615, 'compression_ratio': 1.6442953020134228, 'no_speech_prob': 0.0005702855996787548}, {'id': 158, 'seek': 62888, 'start': 651.36, 'end': 655.2, 'text': ' we all know that unsupervised learning is the ultimate answer.', 'tokens': [51488, 321, 439, 458, 300, 2693, 12879, 24420, 2539, 307, 264, 9705, 1867, 13, 51680], 'temperature': 0.0, 'avg_logprob': -0.10986096628250615, 'compression_ratio': 1.6442953020134228, 'no_speech_prob': 0.0005702855996787548}, {'id': 159, 'seek': 62888, 'start': 655.84, 'end': 658.64, 'text': ' So, I guess we better keep working on it.', 'tokens': [51712, 407, 11, 286, 2041, 321, 1101, 1066, 1364, 322, 309, 13, 51852], 'temperature': 0.0, 'avg_logprob': -0.10986096628250615, 'compression_ratio': 1.6442953020134228, 'no_speech_prob': 0.0005702855996787548}, {'id': 160, 'seek': 65864, 'start': 658.64, 'end': 662.24, 'text': ' Unsupervised learning is a huge area of active research.', 'tokens': [50364, 25017, 12879, 24420, 2539, 307, 257, 2603, 1859, 295, 4967, 2132, 13, 50544], 'temperature': 0.0, 'avg_logprob': -0.0798314128603254, 'compression_ratio': 1.673758865248227, 'no_speech_prob': 0.0002234039711765945}, {'id': 161, 'seek': 65864, 'start': 662.24, 'end': 667.68, 'text': ' The human brain is specially designed for this kind of learning and has different parts for vision,', 'tokens': [50544, 440, 1952, 3567, 307, 22549, 4761, 337, 341, 733, 295, 2539, 293, 575, 819, 3166, 337, 5201, 11, 50816], 'temperature': 0.0, 'avg_logprob': -0.0798314128603254, 'compression_ratio': 1.673758865248227, 'no_speech_prob': 0.0002234039711765945}, {'id': 162, 'seek': 65864, 'start': 667.68, 'end': 669.84, 'text': ' language, movement, and so on.', 'tokens': [50816, 2856, 11, 3963, 11, 293, 370, 322, 13, 50924], 'temperature': 0.0, 'avg_logprob': -0.0798314128603254, 'compression_ratio': 1.673758865248227, 'no_speech_prob': 0.0002234039711765945}, {'id': 163, 'seek': 65864, 'start': 670.4, 'end': 676.24, 'text': ' These structures and what kinds of patterns our brains look for were developed over billions of', 'tokens': [50952, 1981, 9227, 293, 437, 3685, 295, 8294, 527, 15442, 574, 337, 645, 4743, 670, 17375, 295, 51244], 'temperature': 0.0, 'avg_logprob': -0.0798314128603254, 'compression_ratio': 1.673758865248227, 'no_speech_prob': 0.0002234039711765945}, {'id': 164, 'seek': 65864, 'start': 676.24, 'end': 677.68, 'text': ' years of evolution.', 'tokens': [51244, 924, 295, 9303, 13, 51316], 'temperature': 0.0, 'avg_logprob': -0.0798314128603254, 'compression_ratio': 1.673758865248227, 'no_speech_prob': 0.0002234039711765945}, {'id': 165, 'seek': 65864, 'start': 677.68, 'end': 681.92, 'text': \" But it's really tricky to build an AI that does unsupervised learning well,\", 'tokens': [51316, 583, 309, 311, 534, 12414, 281, 1322, 364, 7318, 300, 775, 2693, 12879, 24420, 2539, 731, 11, 51528], 'temperature': 0.0, 'avg_logprob': -0.0798314128603254, 'compression_ratio': 1.673758865248227, 'no_speech_prob': 0.0002234039711765945}, {'id': 166, 'seek': 65864, 'start': 681.92, 'end': 685.84, 'text': \" because AI systems can't learn exactly like humans often do,\", 'tokens': [51528, 570, 7318, 3652, 393, 380, 1466, 2293, 411, 6255, 2049, 360, 11, 51724], 'temperature': 0.0, 'avg_logprob': -0.0798314128603254, 'compression_ratio': 1.673758865248227, 'no_speech_prob': 0.0002234039711765945}, {'id': 167, 'seek': 65864, 'start': 685.84, 'end': 687.76, 'text': ' just by watching and imitating.', 'tokens': [51724, 445, 538, 1976, 293, 566, 16350, 13, 51820], 'temperature': 0.0, 'avg_logprob': -0.0798314128603254, 'compression_ratio': 1.673758865248227, 'no_speech_prob': 0.0002234039711765945}, {'id': 168, 'seek': 68776, 'start': 687.76, 'end': 693.6, 'text': ' Someone like us has to design the models and tell them how to look for patterns before turning them loose.', 'tokens': [50364, 8734, 411, 505, 575, 281, 1715, 264, 5245, 293, 980, 552, 577, 281, 574, 337, 8294, 949, 6246, 552, 9612, 13, 50656], 'temperature': 0.0, 'avg_logprob': -0.09526278909328764, 'compression_ratio': 1.6101694915254237, 'no_speech_prob': 0.0009398197289556265}, {'id': 169, 'seek': 68776, 'start': 693.6, 'end': 699.04, 'text': \" Next time, we'll look at applying similar concepts to AI systems that find patterns in words and\", 'tokens': [50656, 3087, 565, 11, 321, 603, 574, 412, 9275, 2531, 10392, 281, 7318, 3652, 300, 915, 8294, 294, 2283, 293, 50928], 'temperature': 0.0, 'avg_logprob': -0.09526278909328764, 'compression_ratio': 1.6101694915254237, 'no_speech_prob': 0.0009398197289556265}, {'id': 170, 'seek': 68776, 'start': 699.04, 'end': 702.88, 'text': \" language and what's called natural language processing.\", 'tokens': [50928, 2856, 293, 437, 311, 1219, 3303, 2856, 9007, 13, 51120], 'temperature': 0.0, 'avg_logprob': -0.09526278909328764, 'compression_ratio': 1.6101694915254237, 'no_speech_prob': 0.0009398197289556265}, {'id': 171, 'seek': 68776, 'start': 703.4399999999999, 'end': 704.3199999999999, 'text': ' See you then.', 'tokens': [51148, 3008, 291, 550, 13, 51192], 'temperature': 0.0, 'avg_logprob': -0.09526278909328764, 'compression_ratio': 1.6101694915254237, 'no_speech_prob': 0.0009398197289556265}, {'id': 172, 'seek': 68776, 'start': 704.3199999999999, 'end': 707.36, 'text': ' Thanks to Wix for supporting PBS Digital Studios.', 'tokens': [51192, 2561, 281, 343, 970, 337, 7231, 33517, 15522, 23005, 13, 51344], 'temperature': 0.0, 'avg_logprob': -0.09526278909328764, 'compression_ratio': 1.6101694915254237, 'no_speech_prob': 0.0009398197289556265}, {'id': 173, 'seek': 68776, 'start': 707.36, 'end': 710.64, 'text': \" Check out Wix.com if you're looking to make your own website.\", 'tokens': [51344, 6881, 484, 343, 970, 13, 1112, 498, 291, 434, 1237, 281, 652, 428, 1065, 3144, 13, 51508], 'temperature': 0.0, 'avg_logprob': -0.09526278909328764, 'compression_ratio': 1.6101694915254237, 'no_speech_prob': 0.0009398197289556265}, {'id': 174, 'seek': 68776, 'start': 710.64, 'end': 715.52, 'text': ' Wix is a platform that allows you to build a personalized website for almost any purpose,', 'tokens': [51508, 343, 970, 307, 257, 3663, 300, 4045, 291, 281, 1322, 257, 28415, 3144, 337, 1920, 604, 4334, 11, 51752], 'temperature': 0.0, 'avg_logprob': -0.09526278909328764, 'compression_ratio': 1.6101694915254237, 'no_speech_prob': 0.0009398197289556265}, {'id': 175, 'seek': 71552, 'start': 715.52, 'end': 721.4399999999999, 'text': ' from promoting your business or creating an online shop to giving you a place for you to test out new ideas.', 'tokens': [50364, 490, 16383, 428, 1606, 420, 4084, 364, 2950, 3945, 281, 2902, 291, 257, 1081, 337, 291, 281, 1500, 484, 777, 3487, 13, 50660], 'temperature': 0.0, 'avg_logprob': -0.14113996922969818, 'compression_ratio': 1.6816720257234727, 'no_speech_prob': 0.06950034946203232}, {'id': 176, 'seek': 71552, 'start': 721.4399999999999, 'end': 726.16, 'text': ' Their technology allows you to create something unique no matter your skill level with templates', 'tokens': [50660, 6710, 2899, 4045, 291, 281, 1884, 746, 3845, 572, 1871, 428, 5389, 1496, 365, 21165, 50896], 'temperature': 0.0, 'avg_logprob': -0.14113996922969818, 'compression_ratio': 1.6816720257234727, 'no_speech_prob': 0.06950034946203232}, {'id': 177, 'seek': 71552, 'start': 726.16, 'end': 727.92, 'text': ' and all-in-one management.', 'tokens': [50896, 293, 439, 12, 259, 12, 546, 4592, 13, 50984], 'temperature': 0.0, 'avg_logprob': -0.14113996922969818, 'compression_ratio': 1.6816720257234727, 'no_speech_prob': 0.06950034946203232}, {'id': 178, 'seek': 71552, 'start': 727.92, 'end': 731.12, 'text': \" If you'd like to check it out, you can go to wix.com.com.\", 'tokens': [50984, 759, 291, 1116, 411, 281, 1520, 309, 484, 11, 291, 393, 352, 281, 261, 970, 13, 1112, 13, 1112, 13, 51144], 'temperature': 0.0, 'avg_logprob': -0.14113996922969818, 'compression_ratio': 1.6816720257234727, 'no_speech_prob': 0.06950034946203232}, {'id': 179, 'seek': 71552, 'start': 731.12, 'end': 734.64, 'text': ' Go slash crash course or click the link in the description.', 'tokens': [51144, 1037, 17330, 8252, 1164, 420, 2052, 264, 2113, 294, 264, 3855, 13, 51320], 'temperature': 0.0, 'avg_logprob': -0.14113996922969818, 'compression_ratio': 1.6816720257234727, 'no_speech_prob': 0.06950034946203232}, {'id': 180, 'seek': 71552, 'start': 734.64, 'end': 739.04, 'text': ' Crash Course AI is produced in association with PBS Digital Studios.', 'tokens': [51320, 31787, 27327, 7318, 307, 7126, 294, 14598, 365, 33517, 15522, 23005, 13, 51540], 'temperature': 0.0, 'avg_logprob': -0.14113996922969818, 'compression_ratio': 1.6816720257234727, 'no_speech_prob': 0.06950034946203232}, {'id': 181, 'seek': 71552, 'start': 739.04, 'end': 741.76, 'text': ' If you want to help keep Crash Course free for everyone forever,', 'tokens': [51540, 759, 291, 528, 281, 854, 1066, 31787, 27327, 1737, 337, 1518, 5680, 11, 51676], 'temperature': 0.0, 'avg_logprob': -0.14113996922969818, 'compression_ratio': 1.6816720257234727, 'no_speech_prob': 0.06950034946203232}, {'id': 182, 'seek': 71552, 'start': 741.76, 'end': 744.4, 'text': ' you can join our community on Patreon.', 'tokens': [51676, 291, 393, 3917, 527, 1768, 322, 15692, 13, 51808], 'temperature': 0.0, 'avg_logprob': -0.14113996922969818, 'compression_ratio': 1.6816720257234727, 'no_speech_prob': 0.06950034946203232}, {'id': 183, 'seek': 74440, 'start': 744.4, 'end': 747.52, 'text': ' And if you want to learn more about the math behind K-Means clustering,', 'tokens': [50364, 400, 498, 291, 528, 281, 1466, 544, 466, 264, 5221, 2261, 591, 12, 12671, 599, 596, 48673, 11, 50520], 'temperature': 0.0, 'avg_logprob': -0.24679036438465118, 'compression_ratio': 1.150943396226415, 'no_speech_prob': 0.0019876076839864254}, {'id': 184, 'seek': 74440, 'start': 747.52, 'end': 750.9599999999999, 'text': ' check out this video from Crash Course Statistics.', 'tokens': [50520, 1520, 484, 341, 960, 490, 31787, 27327, 49226, 13, 50692], 'temperature': 0.0, 'avg_logprob': -0.24679036438465118, 'compression_ratio': 1.150943396226415, 'no_speech_prob': 0.0019876076839864254}], 'language': 'en'}  Thanks to Wix for supporting PBS Digital Studios. Hey, I'm Jabruro and welcome to Crash Course AI. So far in the series, we focused on artificial intelligence that uses supervised learning. These programs need a teacher to use labeled data to tell them right from wrong. And we humans have places where supervised learning happens, like classrooms with teachers. But that's not the only way we learn. We can also learn lots of things on our own by finding patterns in the world. We can look at dogs and elephants and know they're different animals without anyone telling us. Or we can even figure out the rules of a sport just by watching other people play. This kind of learning without a teacher is called unsupervised learning. And in some cases, computers can do it too. The key difference between supervised and unsupervised learning is what we're trying to predict. In supervised learning, we're trying to build a model to predict an answer or label provided by a teacher. In unsupervised learning, instead of a teacher, the world around us is basically providing training labels. For example, if I freeze this video of a tennis ball right now, can you draw what could be the next frame? Unsupervised learning is about modeling the world by guessing like this. And it's useful because we don't need labels provided by a teacher. Babies do a lot of unsupervised learning by watching and imitating people. And we'd like computers to be able to learn like this as well. This lets us utilize lots of freely available data in the world or on the internet. In many cases, one of the easiest ways to understand how AI can use unsupervised learning is by doing it ourselves. So let's look at a few photos of flowers with no labels. The most basic way to model the world is to assume that it's made up of distinct groups of objects that share properties. So, for example, how many types of flowers are here? We can say there are two because there are two colors, purple and yellow. Or we can look at the petal shapes and divide them into round petals and tall vertical ones. Or maybe we have some more experience with flowers and realize that two of these are tulips, one is a sunflower, and the last one is a daisy. So there are three categories. Immediately recognizing different properties like this and creating categories is called unsupervised clustering. We don't have labels provided by a teacher, but we do have a key assumption about the world that we're modeling. Certain objects are more similar to each other than others. We can program computers to perform clustering. But to do that, we need to choose a few properties of flowers we're interested in looking at, like how we picked color or shape just now. For a more realistic example, let's say I bought a packet of iris seeds to plant in my garden. After the flowers bloom, though, it looks like there were several different species of irises mixed up in that one packet. Now, I'm no expert gardener, but I can use some AI to help me analyze my garden. To construct the model, we have to answer two key questions. First, what observation can we measure? All of these flowers are purple, so that's probably not the best way to tell them apart. But different irises seem to have different petal lengths and widths, which we can measure and place on this graph with petal length on the y-axis and width on the x-axis. And second, how do we want to represent the world? We're going to stick to a very simple assumption here. There are clusters in our data. Specifically, we're going to say there are some number of groups called K clusters, but we don't know where they are. To help us, we're going to use the K-means clustering algorithm. K-means clustering is a simple algorithm. All it needs is a way to compare observations, a way to guess how many clusters exist in the data, and a way to calculate averages for each cluster it predicts. In particular, we want to calculate the mean by adding up all data points in a cluster and dividing by the total number of points. Remember, unsupervised learning is about modeling the world, so our algorithm will have two steps. First, our AI will predict. What does the model expect the world to look like? In other words, which flowers should be clustered together because they're the same species? Second, our AI will correct or learn. The model will update its belief to agree with its observation of the world. To start the process, we have to specify how many clusters the model should look for. I'm guessing there are three clusters in the data, so that becomes the model's initial understanding of the world. And we're looking for K equals 3 averages or 3 types of viruses. But to start, our model doesn't really know anything. So the averages are random and so are its predictions. Each data point, which is a flower, is given a label as type 1, type 2, or type 3. Based on the algorithm's beliefs. Next, our model tries to correct itself. The average of each cluster of data points should be in the middle. So the model corrects itself by calculating new averages. We can see those averages here, marked with X's, which gives us an updated model of the three, or so we guessed, types of viruses. But the graph is still pretty noisy. For example, it's a little weird that we have type 2 flowers so close to the average for type 3 flowers. But we did start with a random model, so we can't expect too much accuracy. Logically, we know that irises of the same species tend to have similar petals. So those data points should be clustered together. Since we did a correction or learning step, we can repeat the process, starting with a new prediction step. Let's predict new labels using the X's that mark the averages of each label. We'll give each data point the label of its closest X. Type 1, type 2, or type 3. And then we'll calculate new averages. Ah, that's better. But still not the cleanest clusters. So we can repeat the process again. Predict, learn, predict, learn. Eventually, the X's will stop moving and we will have a model of iris clusters created with unsupervised learning. Now, the ultimate question is, did we find meaningful patterns about the world with our AI? We made an assumption that there were three types of irises and we assumed that they had different petal lengths and widths. Was this true? Lucky for us, I have a friend who is a master gardener. I showed him the real life flowers closest to each of the three averages and he said that type 1 is versicolor, type 2 is satosa, and type 3 is virginica. Three different iris species. We learned about the world from observation, which is what makes this unsupervised learning. Even though we relied a tiny bit on a teacher, the master gardener, for confirmation and help. Now that we've learned the basics, we can experiment with harder examples. Let's say we want to use unsupervised learning to sort a bunch of different photos, and not just three iris species. First, what observations can we measure? How much green there is, whether there's a nose and fur? To have a computer make these observations, we need to measure thousands of red, green, and blue pixels in each image. Second, how do we want to represent the world? Before, we were only working with two features, so we could just use the averages of the cluster data points and get meaningful extraction from it. But when dealing with images, we can't use the same method, because we won't get much meaning out of averaging colored pixels for what we want to accomplish. Somehow, we need the model to create a representation that tells us if two images are similar. There are meaningful patterns in the data that are more abstract than individual pixels, and finding them across many images is what's called representation learning. These patterns help us understand what's in the images and how to compare them to each other. Representation learning happens both in supervised and unsupervised learning models, so we can do it with or without labels to find patterns in the world. To understand the basic idea of representation learning, check out this experiment. I'm going to look at a picture really fast and then try and draw it. Ready, set, go. My eyes took in the picture and remembered important features, so I'm building a representation in my mind. But I can't just show you my thoughts to get feedback on what parts I misremembered, so I have to produce a reconstruction or draw the original image from my memory. Alright, so this is what I've got. Now let's compare my drawing to the original image. Let's see, round plate, triangle slice of pizza, some cheese, some crust, tablecloth. Pretty good. For an AI, making a reconstruction would mean producing all the right pixel values to make a reconstruction. Arcane means clustering algorithm from before predicted classes for flowers based on how close the data points were to the averages. For images, we will have learned image representations instead of averages. After that step, just like before, the AI will have to correct itself. Previously, we updated the K clusters based on how well our predicted labels fit the data, but for images, we'd have to update the model's internal representations based on its reconstructions. There are different ways to use unsupervised learning in combination with representation learning so that an AI can compare images. Like for example, there's a type of neural network called an autoencoder, which uses the same basic principles of weights and biases to process inputs, pass data onto the hidden layers, and finally to a prediction output layer. If John Greenbott was programmed with an autoencoder, the input would be an image, the hidden layers would contain representations, and the output would be a full reconstruction of the original image. Which gets more accurate the more we train this AI. Theoretically, I could give John Greenbott a representation of a pizza and he could reconstruct the original pizza image. What's so powerful about unsupervised learning is that the world is our teacher. By looking around, taking in a lot of data, and predicting what we'll see next, we learn about how the world works and how it should be represented. When asked how AI will fulfill its grand ambitions, 2018 Turing Award winning professor, Jan LeCun said, we all know that unsupervised learning is the ultimate answer. So, I guess we better keep working on it. Unsupervised learning is a huge area of active research. The human brain is specially designed for this kind of learning and has different parts for vision, language, movement, and so on. These structures and what kinds of patterns our brains look for were developed over billions of years of evolution. But it's really tricky to build an AI that does unsupervised learning well, because AI systems can't learn exactly like humans often do, just by watching and imitating. Someone like us has to design the models and tell them how to look for patterns before turning them loose. Next time, we'll look at applying similar concepts to AI systems that find patterns in words and language and what's called natural language processing. See you then. Thanks to Wix for supporting PBS Digital Studios. Check out Wix.com if you're looking to make your own website. Wix is a platform that allows you to build a personalized website for almost any purpose, from promoting your business or creating an online shop to giving you a place for you to test out new ideas. Their technology allows you to create something unique no matter your skill level with templates and all-in-one management. If you'd like to check it out, you can go to wix.com.com. Go slash crash course or click the link in the description. Crash Course AI is produced in association with PBS Digital Studios. If you want to help keep Crash Course free for everyone forever, you can join our community on Patreon. And if you want to learn more about the math behind K-Means clustering, check out this video from Crash Course Statistics.\n"
          ]
        }
      ],
      "source": [
        "# pip install -U openai-whisper\n",
        "# pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "import whisper\n",
        "\n",
        "video_path = f'/content/{yt.title}.mp4'.replace('#', '').replace(':', '')\n",
        "# print(video_path == '/content/Unsupervised Learning Crash Course AI 6.mp4')\n",
        "# print(video_path)\n",
        "# print('/content/Unsupervised Learning Crash Course AI 6.mp4')\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model('small')\n",
        "print(\"Whisper model loaded successfully\")\n",
        "\n",
        "video = VideoFileClip(video_path)\n",
        "audio = video.audio\n",
        "audio.write_audiofile('/content/audio.wav')\n",
        "\n",
        "# Transcribe the audio using Whisper\n",
        "result = whisper.transcribe(model, '/content/audio.wav')\n",
        "\n",
        "print(result, result['text'])\n",
        "fhandle = open('transcribed_audio.txt', 'w')\n",
        "fhandle.write(result['text'])\n",
        "fhandle.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsivZ0P0h3HZ"
      },
      "source": [
        "# Generated Transcription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "cxFDAUext_ze",
        "outputId": "ae6729be-55a1-4a39-912d-99762deda315"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Thanks to Wix for supporting PBS Digital Studios. Hey, I'm Jabruro and welcome to Crash Course AI. So far in the series, we focused on artificial intelligence that uses supervised learning. These programs need a teacher to use labeled data to tell them right from wrong. And we humans have places where supervised learning happens, like classrooms with teachers. But that's not the only way we learn. We can also learn lots of things on our own by finding patterns in the world. We can look at dogs and elephants and know they're different animals without anyone telling us. Or we can even figure out the rules of a sport just by watching other people play. This kind of learning without a teacher is called unsupervised learning. And in some cases, computers can do it too. The key difference between supervised and unsupervised learning is what we're trying to predict. In supervised learning, we're trying to build a model to predict an answer or label provided by a teacher. In unsupervised learning, instead of a teacher, the world around us is basically providing training labels. For example, if I freeze this video of a tennis ball right now, can you draw what could be the next frame? Unsupervised learning is about modeling the world by guessing like this. And it's useful because we don't need labels provided by a teacher. Babies do a lot of unsupervised learning by watching and imitating people. And we'd like computers to be able to learn like this as well. This lets us utilize lots of freely available data in the world or on the internet. In many cases, one of the easiest ways to understand how AI can use unsupervised learning is by doing it ourselves. So let's look at a few photos of flowers with no labels. The most basic way to model the world is to assume that it's made up of distinct groups of objects that share properties. So, for example, how many types of flowers are here? We can say there are two because there are two colors, purple and yellow. Or we can look at the petal shapes and divide them into round petals and tall vertical ones. Or maybe we have some more experience with flowers and realize that two of these are tulips, one is a sunflower, and the last one is a daisy. So there are three categories. Immediately recognizing different properties like this and creating categories is called unsupervised clustering. We don't have labels provided by a teacher, but we do have a key assumption about the world that we're modeling. Certain objects are more similar to each other than others. We can program computers to perform clustering. But to do that, we need to choose a few properties of flowers we're interested in looking at, like how we picked color or shape just now. For a more realistic example, let's say I bought a packet of iris seeds to plant in my garden. After the flowers bloom, though, it looks like there were several different species of irises mixed up in that one packet. Now, I'm no expert gardener, but I can use some AI to help me analyze my garden. To construct the model, we have to answer two key questions. First, what observation can we measure? All of these flowers are purple, so that's probably not the best way to tell them apart. But different irises seem to have different petal lengths and widths, which we can measure and place on this graph with petal length on the y-axis and width on the x-axis. And second, how do we want to represent the world? We're going to stick to a very simple assumption here. There are clusters in our data. Specifically, we're going to say there are some number of groups called K clusters, but we don't know where they are. To help us, we're going to use the K-means clustering algorithm. K-means clustering is a simple algorithm. All it needs is a way to compare observations, a way to guess how many clusters exist in the data, and a way to calculate averages for each cluster it predicts. In particular, we want to calculate the mean by adding up all data points in a cluster and dividing by the total number of points. Remember, unsupervised learning is about modeling the world, so our algorithm will have two steps. First, our AI will predict. What does the model expect the world to look like? In other words, which flowers should be clustered together because they're the same species? Second, our AI will correct or learn. The model will update its belief to agree with its observation of the world. To start the process, we have to specify how many clusters the model should look for. I'm guessing there are three clusters in the data, so that becomes the model's initial understanding of the world. And we're looking for K equals 3 averages or 3 types of viruses. But to start, our model doesn't really know anything. So the averages are random and so are its predictions. Each data point, which is a flower, is given a label as type 1, type 2, or type 3. Based on the algorithm's beliefs. Next, our model tries to correct itself. The average of each cluster of data points should be in the middle. So the model corrects itself by calculating new averages. We can see those averages here, marked with X's, which gives us an updated model of the three, or so we guessed, types of viruses. But the graph is still pretty noisy. For example, it's a little weird that we have type 2 flowers so close to the average for type 3 flowers. But we did start with a random model, so we can't expect too much accuracy. Logically, we know that irises of the same species tend to have similar petals. So those data points should be clustered together. Since we did a correction or learning step, we can repeat the process, starting with a new prediction step. Let's predict new labels using the X's that mark the averages of each label. We'll give each data point the label of its closest X. Type 1, type 2, or type 3. And then we'll calculate new averages. Ah, that's better. But still not the cleanest clusters. So we can repeat the process again. Predict, learn, predict, learn. Eventually, the X's will stop moving and we will have a model of iris clusters created with unsupervised learning. Now, the ultimate question is, did we find meaningful patterns about the world with our AI? We made an assumption that there were three types of irises and we assumed that they had different petal lengths and widths. Was this true? Lucky for us, I have a friend who is a master gardener. I showed him the real life flowers closest to each of the three averages and he said that type 1 is versicolor, type 2 is satosa, and type 3 is virginica. Three different iris species. We learned about the world from observation, which is what makes this unsupervised learning. Even though we relied a tiny bit on a teacher, the master gardener, for confirmation and help. Now that we've learned the basics, we can experiment with harder examples. Let's say we want to use unsupervised learning to sort a bunch of different photos, and not just three iris species. First, what observations can we measure? How much green there is, whether there's a nose and fur? To have a computer make these observations, we need to measure thousands of red, green, and blue pixels in each image. Second, how do we want to represent the world? Before, we were only working with two features, so we could just use the averages of the cluster data points and get meaningful extraction from it. But when dealing with images, we can't use the same method, because we won't get much meaning out of averaging colored pixels for what we want to accomplish. Somehow, we need the model to create a representation that tells us if two images are similar. There are meaningful patterns in the data that are more abstract than individual pixels, and finding them across many images is what's called representation learning. These patterns help us understand what's in the images and how to compare them to each other. Representation learning happens both in supervised and unsupervised learning models, so we can do it with or without labels to find patterns in the world. To understand the basic idea of representation learning, check out this experiment. I'm going to look at a picture really fast and then try and draw it. Ready, set, go. My eyes took in the picture and remembered important features, so I'm building a representation in my mind. But I can't just show you my thoughts to get feedback on what parts I misremembered, so I have to produce a reconstruction or draw the original image from my memory. Alright, so this is what I've got. Now let's compare my drawing to the original image. Let's see, round plate, triangle slice of pizza, some cheese, some crust, tablecloth. Pretty good. For an AI, making a reconstruction would mean producing all the right pixel values to make a reconstruction. Arcane means clustering algorithm from before predicted classes for flowers based on how close the data points were to the averages. For images, we will have learned image representations instead of averages. After that step, just like before, the AI will have to correct itself. Previously, we updated the K clusters based on how well our predicted labels fit the data, but for images, we'd have to update the model's internal representations based on its reconstructions. There are different ways to use unsupervised learning in combination with representation learning so that an AI can compare images. Like for example, there's a type of neural network called an autoencoder, which uses the same basic principles of weights and biases to process inputs, pass data onto the hidden layers, and finally to a prediction output layer. If John Greenbott was programmed with an autoencoder, the input would be an image, the hidden layers would contain representations, and the output would be a full reconstruction of the original image. Which gets more accurate the more we train this AI. Theoretically, I could give John Greenbott a representation of a pizza and he could reconstruct the original pizza image. What's so powerful about unsupervised learning is that the world is our teacher. By looking around, taking in a lot of data, and predicting what we'll see next, we learn about how the world works and how it should be represented. When asked how AI will fulfill its grand ambitions, 2018 Turing Award winning professor, Jan LeCun said, we all know that unsupervised learning is the ultimate answer. So, I guess we better keep working on it. Unsupervised learning is a huge area of active research. The human brain is specially designed for this kind of learning and has different parts for vision, language, movement, and so on. These structures and what kinds of patterns our brains look for were developed over billions of years of evolution. But it's really tricky to build an AI that does unsupervised learning well, because AI systems can't learn exactly like humans often do, just by watching and imitating. Someone like us has to design the models and tell them how to look for patterns before turning them loose. Next time, we'll look at applying similar concepts to AI systems that find patterns in words and language and what's called natural language processing. See you then. Thanks to Wix for supporting PBS Digital Studios. Check out Wix.com if you're looking to make your own website. Wix is a platform that allows you to build a personalized website for almost any purpose, from promoting your business or creating an online shop to giving you a place for you to test out new ideas. Their technology allows you to create something unique no matter your skill level with templates and all-in-one management. If you'd like to check it out, you can go to wix.com.com. Go slash crash course or click the link in the description. Crash Course AI is produced in association with PBS Digital Studios. If you want to help keep Crash Course free for everyone forever, you can join our community on Patreon. And if you want to learn more about the math behind K-Means clustering, check out this video from Crash Course Statistics.\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-TBNkjmTcpZ"
      },
      "source": [
        "# MCQ Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwjA8lupwgNJ",
        "outputId": "e7d67c6e-ca41-4180-9cfa-5080b49f2f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting simplet5\n",
            "  Downloading simplet5-0.1.4.tar.gz (7.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simplet5) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simplet5) (2.0.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simplet5) (0.1.99)\n",
            "Requirement already satisfied: torch!=1.8.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from simplet5) (2.3.0+cu121)\n",
            "Collecting transformers==4.16.2 (from simplet5)\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.5.10 (from simplet5)\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.7/527.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10->simplet5) (0.18.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10->simplet5) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10->simplet5) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10->simplet5) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10->simplet5) (2.15.2)\n",
            "Collecting torchmetrics>=0.4.1 (from pytorch-lightning==1.5.10->simplet5)\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1 (from pytorch-lightning==1.5.10->simplet5)\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10->simplet5) (24.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10->simplet5) (4.12.1)\n",
            "Collecting setuptools==59.5.0 (from pytorch-lightning==1.5.10->simplet5)\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->simplet5) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->simplet5) (0.23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->simplet5) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->simplet5) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.16.2->simplet5)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->simplet5) (0.19.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8.0,>=1.7.0->simplet5) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.8.0,>=1.7.0->simplet5) (12.5.40)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->simplet5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simplet5) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simplet5) (2024.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10->simplet5) (3.9.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->simplet5) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (3.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->simplet5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->simplet5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->simplet5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->simplet5) (2024.6.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.5.10->simplet5)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8.0,>=1.7.0->simplet5) (2.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.2->simplet5) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.2->simplet5) (1.4.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8.0,>=1.7.0->simplet5) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10->simplet5) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10->simplet5) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10->simplet5) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10->simplet5) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10->simplet5) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10->simplet5) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (3.2.2)\n",
            "Building wheels for collected packages: simplet5\n",
            "  Building wheel for simplet5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplet5: filename=simplet5-0.1.4-py3-none-any.whl size=6854 sha256=059a1d06023ab13192743cefb884418204aa13e1bd8fe7e35d5d147dbb162468\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7d/af/743765400878438a7593f13f89fdf4004dcde0f2a8e6cb6684\n",
            "Successfully built simplet5\n",
            "Installing collected packages: setuptools, sacremoses, pyDeprecate, lightning-utilities, transformers, torchmetrics, pytorch-lightning, simplet5\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n"
          ]
        }
      ],
      "source": [
        "!pip install simplet5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nleujbWFwtIK"
      },
      "outputs": [],
      "source": [
        "from simplet5 import SimpleT5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhHf0UWhEuhR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXy3JI4sxP6i"
      },
      "source": [
        "**Loading Both Question and Mcq-Genrate Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtZ4QkjBwt0k"
      },
      "outputs": [],
      "source": [
        "generate_question = SimpleT5()\n",
        "generate_mcq_option=SimpleT5()\n",
        "\n",
        "path1 = \"/content/drive/MyDrive/FYP work/mcq models and code/trained-t5-mcq-generate-model\"\n",
        "path2 = \"/content/drive/MyDrive/FYP work/mcq models and code/trained-mcq-option-t5\"\n",
        "\n",
        "generate_question.load_model(\"t5\", path1, use_gpu = True)\n",
        "generate_mcq_option.load_model(\"t5\", path2, use_gpu = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOatHEt_z5N9"
      },
      "source": [
        "Function To Generate Mcq-Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4xhtPjqxBYj"
      },
      "outputs": [],
      "source": [
        "def generate_question_mcq(text):\n",
        "  generated_output=generate_question.predict(text)\n",
        "  ques_tion=generated_output[0]\n",
        "  return ques_tion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGaziusiz_p8"
      },
      "source": [
        "Function To Generate Mcq-Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBVvwDjGyIFN"
      },
      "outputs": [],
      "source": [
        "def generate_mcq_options(text):\n",
        "  ques_tion=generate_question_mcq(text)\n",
        "  # print(ques_tion)\n",
        "  mcq_option=generate_mcq_option.predict(ques_tion)\n",
        "  # print(mcq_option[0])\n",
        "  return ques_tion, mcq_option[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRnBj9Z3053C"
      },
      "source": [
        "Inference Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWqisDxycnxA"
      },
      "outputs": [],
      "source": [
        "from simplet5 import SimpleT5\n",
        "\n",
        "model = SimpleT5()\n",
        "model.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2TTanQgcr78"
      },
      "outputs": [],
      "source": [
        "# let's load the trained model for inferencing:\n",
        "model.load_model(\"t5\",\"t5-base\", use_gpu=True)\n",
        "\n",
        "ftranscript = open('transcribed_audio.txt', 'r')\n",
        "transcript = ''\n",
        "\n",
        "for line in ftranscript:\n",
        "  transcript += line\n",
        "\n",
        "text = 'summarize : ' + transcript\n",
        "model.predict(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4cNGZwOxxSJ"
      },
      "outputs": [],
      "source": [
        "#remeber to add \"Question: \" tag before the text\n",
        "ftranscript = open('transcribed_audio.txt', 'r')\n",
        "transcript = ''\n",
        "\n",
        "for line in ftranscript:\n",
        "  transcript += line\n",
        "\n",
        "text = 'Question : ' + transcript\n",
        "\n",
        "q, mcqs = generate_mcq_options(text)\n",
        "q, mcqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlVim4qUKeRl"
      },
      "outputs": [],
      "source": [
        "question = q.split('?')[0] + '?'\n",
        "answer = q.split('?')[1].split(':')[1].strip()\n",
        "\n",
        "write = question\n",
        "\n",
        "question, answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1kPZBInK-GB"
      },
      "outputs": [],
      "source": [
        "mcqs = mcqs.split('Option')[1:]\n",
        "write += '\\n' + '\\n'.join(mcqs)\n",
        "# mcqs\n",
        "print(write)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm8iTBnYK1G4"
      },
      "outputs": [],
      "source": [
        "# Function to find all close matches of\n",
        "# input string in given list of possible strings\n",
        "from difflib import get_close_matches\n",
        "\n",
        "def closeMatches(patterns, word):\n",
        "\treturn get_close_matches(word, patterns)\n",
        "\n",
        "answer = 'Correct Option : ' + closeMatches(mcqs, answer)[0].split(':')[0].strip()\n",
        "write += '\\n' + answer\n",
        "print(write)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJa8jjfONKRf"
      },
      "outputs": [],
      "source": [
        "fhandle = open('mcqs.txt', 'w')\n",
        "fhandle.write(write)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSrZ2NqCYIvY"
      },
      "source": [
        "In the next iteration we'll try to improve mcq qualtiy by using falcon 7b model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFzslI1FVzYi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}