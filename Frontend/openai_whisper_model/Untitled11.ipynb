{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi_KKZocSF3i",
        "outputId": "44bc2a54-50d4-49f1-8468-807d0edfd8b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_99', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "50/50 [==============================] - 467s 9s/step - loss: 1.0556 - accuracy: 0.4161\n",
            "Epoch 2/5\n",
            "50/50 [==============================] - 447s 9s/step - loss: 0.9477 - accuracy: 0.4994\n",
            "Epoch 3/5\n",
            "50/50 [==============================] - 444s 9s/step - loss: 0.9148 - accuracy: 0.5120\n",
            "Epoch 4/5\n",
            "50/50 [==============================] - 450s 9s/step - loss: 0.8948 - accuracy: 0.5322\n",
            "Epoch 5/5\n",
            "50/50 [==============================] - 443s 9s/step - loss: 0.8802 - accuracy: 0.5485\n",
            "Test Accuracy: 0.4673366844654083\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the labeled MCQs dataset from an Excel file\n",
        "df = pd.read_excel('/content/my2.xlsx')\n",
        "mcqs = df.iloc[:, 0].tolist()\n",
        "labels = df.iloc[:, 6].tolist()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(mcqs, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode the labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "test_labels = label_encoder.transform(test_labels)\n",
        "\n",
        "# Initialize the BERT tokenizer and encode the texts\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels))\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=model.hf_compute_loss, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset.shuffle(1000).batch(16), epochs=5, batch_size=16)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "_, accuracy = model.evaluate(test_dataset.batch(16), verbose=0)\n",
        "print('Test Accuracy:', accuracy)\n"
      ]
    }
  ]
}